{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3caa6f30",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"padding-left:50px;\" src=\"figures_wk4/data_cleaning.png\" width=350><br>\n",
    "### User Bias in Data Cleaning\n",
    "For your homework assignment this week, we will explore how our treatment of our data can impact the quality of our results.\n",
    "\n",
    "**Dataset:**\n",
    "The data is a Salary Survey from AskAManager.org. It’s US-centric-ish but does allow for a range of country inputs.\n",
    "\n",
    "A list of the corresponding survey questions can be found [here](https://www.askamanager.org/2021/04/how-much-money-do-you-make-4.html).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960098db-5807-4f89-85b4-be34e2a80484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('survey_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb64b9a-0236-4153-9dc1-560c873d2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28108 entries, 0 to 28107\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   timestamp  28108 non-null  object \n",
      " 1   q1         28108 non-null  object \n",
      " 2   q2         28033 non-null  object \n",
      " 3   q3         28107 non-null  object \n",
      " 4   q4         7273 non-null   object \n",
      " 5   q5         28108 non-null  object \n",
      " 6   q6         20793 non-null  float64\n",
      " 7   q7         28108 non-null  object \n",
      " 8   q8         211 non-null    object \n",
      " 9   q9         3047 non-null   object \n",
      " 10  q10        28108 non-null  object \n",
      " 11  q11        23074 non-null  object \n",
      " 12  q12        28026 non-null  object \n",
      " 13  q13        28108 non-null  object \n",
      " 14  q14        28108 non-null  object \n",
      " 15  q15        27885 non-null  object \n",
      " 16  q16        27937 non-null  object \n",
      " 17  q17        27931 non-null  object \n",
      "dtypes: float64(1), object(17)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59804474-eb37-41f7-b958-1c604a24586d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>q5</th>\n",
       "      <th>q6</th>\n",
       "      <th>q7</th>\n",
       "      <th>q8</th>\n",
       "      <th>q9</th>\n",
       "      <th>q10</th>\n",
       "      <th>q11</th>\n",
       "      <th>q12</th>\n",
       "      <th>q13</th>\n",
       "      <th>q14</th>\n",
       "      <th>q15</th>\n",
       "      <th>q16</th>\n",
       "      <th>q17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/27/2021 11:02:10</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Research and Instruction Librarian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55,000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Boston</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4/27/2021 11:02:22</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Computing or Tech</td>\n",
       "      <td>Change &amp; Internal Communications Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54,600</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>GBP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4/27/2021 11:02:38</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Marketing Specialist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Chattanooga</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/27/2021 11:02:41</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Nonprofits</td>\n",
       "      <td>Program Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62,000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/27/2021 11:02:42</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Accounting Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60,000</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Greenville</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp     q1                             q2  \\\n",
       "0  4/27/2021 11:02:10  25-34   Education (Higher Education)   \n",
       "1  4/27/2021 11:02:22  25-34              Computing or Tech   \n",
       "2  4/27/2021 11:02:38  25-34  Accounting, Banking & Finance   \n",
       "3  4/27/2021 11:02:41  25-34                     Nonprofits   \n",
       "4  4/27/2021 11:02:42  25-34  Accounting, Banking & Finance   \n",
       "\n",
       "                                         q3   q4      q5      q6   q7   q8  \\\n",
       "0        Research and Instruction Librarian  NaN  55,000     0.0  USD  NaN   \n",
       "1  Change & Internal Communications Manager  NaN  54,600  4000.0  GBP  NaN   \n",
       "2                      Marketing Specialist  NaN  34,000     NaN  USD  NaN   \n",
       "3                           Program Manager  NaN  62,000  3000.0  USD  NaN   \n",
       "4                        Accounting Manager  NaN  60,000  7000.0  USD  NaN   \n",
       "\n",
       "    q9             q10             q11          q12           q13  \\\n",
       "0  NaN   United States   Massachusetts       Boston     5-7 years   \n",
       "1  NaN  United Kingdom             NaN    Cambridge  8 - 10 years   \n",
       "2  NaN              US       Tennessee  Chattanooga   2 - 4 years   \n",
       "3  NaN             USA       Wisconsin    Milwaukee  8 - 10 years   \n",
       "4  NaN              US  South Carolina   Greenville  8 - 10 years   \n",
       "\n",
       "           q14              q15         q16    q17  \n",
       "0    5-7 years  Master's degree       Woman  White  \n",
       "1    5-7 years   College degree  Non-binary  White  \n",
       "2  2 - 4 years   College degree       Woman  White  \n",
       "3    5-7 years   College degree       Woman  White  \n",
       "4    5-7 years   College degree       Woman  White  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce58c7-0e14-4426-a4b7-96805ea64bbb",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "Your goal for this assignment is to observe how your data treatment during the cleaning process can skew or bias the dataset.\n",
    "\n",
    "Before diving right in, stop and read through the questions associated with the dataset. As you can see, they are either free-form text entries or categorical selections. Knowing this, perform some exploratory data analysis (EDA) to investigate the \"state\" of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fe5581-c12c-4b1d-bcde-be9bb0998553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28108 entries, 0 to 28107\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   timestamp  28108 non-null  object \n",
      " 1   q1         28108 non-null  object \n",
      " 2   q2         28033 non-null  object \n",
      " 3   q3         28107 non-null  object \n",
      " 4   q4         7273 non-null   object \n",
      " 5   q5         28108 non-null  object \n",
      " 6   q6         20793 non-null  float64\n",
      " 7   q7         28108 non-null  object \n",
      " 8   q8         211 non-null    object \n",
      " 9   q9         3047 non-null   object \n",
      " 10  q10        28108 non-null  object \n",
      " 11  q11        23074 non-null  object \n",
      " 12  q12        28026 non-null  object \n",
      " 13  q13        28108 non-null  object \n",
      " 14  q14        28108 non-null  object \n",
      " 15  q15        27885 non-null  object \n",
      " 16  q16        27937 non-null  object \n",
      " 17  q17        27931 non-null  object \n",
      "dtypes: float64(1), object(17)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01101ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>age</th>\n",
       "      <th>Work_Industry</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Title_Add_Comments</th>\n",
       "      <th>Annual_Salary</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Currency_Other</th>\n",
       "      <th>Income_Context</th>\n",
       "      <th>Country_Work</th>\n",
       "      <th>State_Work_USA</th>\n",
       "      <th>City_Work</th>\n",
       "      <th>Total_Years_of_Experience</th>\n",
       "      <th>Related_Work_Experience</th>\n",
       "      <th>Highest_Qualification</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25522</th>\n",
       "      <td>5/6/2021 18:49:40</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Nonprofits</td>\n",
       "      <td>Research Associate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46000</td>\n",
       "      <td>750.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12178</th>\n",
       "      <td>4/28/2021 8:40:28</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Project Coordinator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Boston</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6846</th>\n",
       "      <td>4/27/2021 14:12:09</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Marketing, Advertising &amp; PR</td>\n",
       "      <td>Global Communications Coordinator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18518</th>\n",
       "      <td>4/29/2021 0:37:28</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Marketing, Advertising &amp; PR</td>\n",
       "      <td>Senior Product Marketing Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210,000</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Man</td>\n",
       "      <td>Asian or Asian American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>4/27/2021 15:11:29</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>Assistant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35,000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300 per closed transaction</td>\n",
       "      <td>US</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>1 year or less</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    age                  Work_Industry  \\\n",
       "25522   5/6/2021 18:49:40  25-34                     Nonprofits   \n",
       "12178   4/28/2021 8:40:28  25-34  Accounting, Banking & Finance   \n",
       "6846   4/27/2021 14:12:09  25-34    Marketing, Advertising & PR   \n",
       "18518   4/29/2021 0:37:28  25-34    Marketing, Advertising & PR   \n",
       "7765   4/27/2021 15:11:29  25-34                    Real Estate   \n",
       "\n",
       "                               Job_Title Job_Title_Add_Comments Annual_Salary  \\\n",
       "25522                 Research Associate                    NaN         46000   \n",
       "12178                Project Coordinator                    NaN        74,000   \n",
       "6846   Global Communications Coordinator                    NaN        52,000   \n",
       "18518   Senior Product Marketing Manager                    NaN       210,000   \n",
       "7765                           Assistant                    NaN        35,000   \n",
       "\n",
       "         Bonus Currency Currency_Other               Income_Context  \\\n",
       "25522    750.0      USD            NaN                          NaN   \n",
       "12178      NaN      USD            NaN                          NaN   \n",
       "6846       NaN      CAD            NaN                          NaN   \n",
       "18518  35000.0      USD            NaN                          NaN   \n",
       "7765     300.0      USD            NaN  300 per closed transaction    \n",
       "\n",
       "        Country_Work State_Work_USA      City_Work Total_Years_of_Experience  \\\n",
       "25522  United States       Virginia      Arlington                 5-7 years   \n",
       "12178            USA  Massachusetts         Boston                 5-7 years   \n",
       "6846          Canada            NaN       Montreal               2 - 4 years   \n",
       "18518  United States     California  San Francisco              8 - 10 years   \n",
       "7765              US      Louisiana    New Orleans               2 - 4 years   \n",
       "\n",
       "      Related_Work_Experience Highest_Qualification Gender  \\\n",
       "25522               5-7 years        College degree  Woman   \n",
       "12178             2 - 4 years        College degree  Woman   \n",
       "6846              2 - 4 years       Master's degree  Woman   \n",
       "18518             2 - 4 years       Master's degree    Man   \n",
       "7765           1 year or less       Master's degree  Woman   \n",
       "\n",
       "                          Race  \n",
       "25522                    White  \n",
       "12178                    White  \n",
       "6846                     White  \n",
       "18518  Asian or Asian American  \n",
       "7765                     White  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_map = {\n",
    "    'q1': 'age',\n",
    "    'q2': 'Work_Industry',\n",
    "    'q3': 'Job_Title',\n",
    "    'q4': 'Job_Title_Add_Comments',\n",
    "    'q5': 'Annual_Salary',\n",
    "    'q6': 'Bonus',\n",
    "    'q7': 'Currency',\n",
    "    'q8': 'Currency_Other',\n",
    "    'q9': 'Income_Context',\n",
    "    'q10': 'Country_Work',\n",
    "    'q11': 'State_Work_USA',\n",
    "    'q12': 'City_Work',\n",
    "    'q13': 'Total_Years_of_Experience',\n",
    "    'q14': 'Related_Work_Experience',\n",
    "    'q15': 'Highest_Qualification',\n",
    "    'q16': 'Gender',\n",
    "    'q17': 'Race'\n",
    "}\n",
    "\n",
    "df.rename(columns=questions_map, inplace=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148dda35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annual_Salary</th>\n",
       "      <th>Bonus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.810800e+04</td>\n",
       "      <td>2.079300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.619324e+05</td>\n",
       "      <td>1.824460e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.619338e+07</td>\n",
       "      <td>8.336249e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.400000e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>2.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.098268e+05</td>\n",
       "      <td>1.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000070e+09</td>\n",
       "      <td>1.200000e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Annual_Salary         Bonus\n",
       "count   2.810800e+04  2.079300e+04\n",
       "mean    3.619324e+05  1.824460e+04\n",
       "std     3.619338e+07  8.336249e+05\n",
       "min     0.000000e+00  0.000000e+00\n",
       "25%     5.400000e+04  0.000000e+00\n",
       "50%     7.500000e+04  2.000000e+03\n",
       "75%     1.098268e+05  1.000000e+04\n",
       "max     6.000070e+09  1.200000e+08"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Strings to Numbers\n",
    "# We need to convert the columns 'Annual_Salary' and 'Bonus' to numbers. We'll also convert the 'Total_Years_of_Experience' column to a number.\n",
    "\n",
    "# Step 1: Remove any commas from the values in the 'Annual_Salary' and 'Bonus' columns.\n",
    "df['Annual_Salary'] = df['Annual_Salary'].str.replace(',', '')\n",
    "\n",
    "# Step 2: Convert the 'Annual_Salary', 'Bonus', and 'Total_Years_of_Experience' columns to numbers.\n",
    "df['Annual_Salary'] = pd.to_numeric(df['Annual_Salary'], errors='coerce')\n",
    "df['Bonus'] = pd.to_numeric(df['Bonus'], errors='coerce')\n",
    "\n",
    "df[['Annual_Salary', 'Bonus']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaec8328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gender</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Highest_Qualification</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Total_Years_of_Experience</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Related_Work_Experience</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Currency</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Race</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Currency_Other</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>State_Work_USA</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Country_Work</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Work_Industry</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Income_Context</td>\n",
       "      <td>2983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>City_Work</td>\n",
       "      <td>4841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Job_Title_Add_Comments</td>\n",
       "      <td>7010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Job_Title</td>\n",
       "      <td>14377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>25326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Column Unique Values\n",
       "14                     Gender             5\n",
       "13      Highest_Qualification             6\n",
       "1                         age             7\n",
       "11  Total_Years_of_Experience             8\n",
       "12    Related_Work_Experience             8\n",
       "5                    Currency            11\n",
       "15                       Race            51\n",
       "6              Currency_Other           124\n",
       "9              State_Work_USA           137\n",
       "8                Country_Work           382\n",
       "2               Work_Industry          1220\n",
       "7              Income_Context          2983\n",
       "10                  City_Work          4841\n",
       "4      Job_Title_Add_Comments          7010\n",
       "3                   Job_Title         14377\n",
       "0                   timestamp         25326"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the distribution of the Category columns\n",
    "unique_values = pd.DataFrame(columns=['Column', 'Unique Values'])\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    new_row = pd.DataFrame({\n",
    "        'Column': [column],\n",
    "        'Unique Values': [df[column].nunique()]\n",
    "    })\n",
    "    unique_values = pd.concat([unique_values, new_row], ignore_index=True)\n",
    "\n",
    "unique_values.sort_values('Unique Values', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c881ba5",
   "metadata": {},
   "source": [
    "`Race` is high unique values count because the question permitted multi-selection or free-text entries, participants provided multiple or unique identifications, resulting in 51 distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60acfc",
   "metadata": {},
   "source": [
    "**1. Currency_Other (124 unique):** A broad range of responses was contributed by free-text entries for alternative or less common currencies. <br>\n",
    "**2. State_Work_USA (137 unique):** Multiple distinct state abbreviations or spellings were introduced, resulting in a high unique count. <br>\n",
    "**3. Country_Work (382 unique):** A globally diverse respondent pool produced significant variation in country names. <br>\n",
    "**4. Work_Industry (1220 unique):** Numerous specialized or niche sectors were captured through free-form descriptions of industries. <br>\n",
    "**5. Income_Context (2983 unique):** Personalized explanations for salary circumstances were provided, creating considerable variability. <br>\n",
    "**6. City_Work (4841 unique):** City names worldwide appeared in various spellings and languages, leading to a large number of distinct entries. <br>\n",
    "**7. Job_Title_Add_Comments (7010 unique):** A wide range of clarifications was added to roles, accounting for extensive variation. <br>\n",
    "**8. Job_Title (14377 unique):** The diversity of occupations was reflected in the broad array of unique job titles reported <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b42cb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp\n",
      "4/27/2021 11:24:33    5\n",
      "4/27/2021 11:30:18    5\n",
      "4/27/2021 11:12:58    5\n",
      "4/27/2021 11:05:08    5\n",
      "4/27/2021 11:05:17    5\n",
      "Name: count, dtype: int64\n",
      "**************************************************\n",
      "age\n",
      "25-34    12668\n",
      "35-44     9908\n",
      "45-54     3193\n",
      "18-24     1236\n",
      "55-64      994\n",
      "Name: count, dtype: int64\n",
      "**************************************************\n",
      "Work_Industry\n",
      "Computing or Tech                       4711\n",
      "Education (Higher Education)            2466\n",
      "Nonprofits                              2420\n",
      "Health care                             1899\n",
      "Government and Public Administration    1893\n",
      "Name: count, dtype: int64\n",
      "**************************************************\n",
      "Job_Title\n",
      "Software Engineer           286\n",
      "Project Manager             230\n",
      "Director                    198\n",
      "Senior Software Engineer    196\n",
      "Program Manager             152\n",
      "Name: count, dtype: int64\n",
      "**************************************************\n",
      "Job_Title_Add_Comments\n",
      "Fundraising                           20\n",
      "In commercial real estate industry    10\n",
      "Attorney                               8\n",
      "Public library                         7\n",
      "Librarian                              7\n",
      "Name: count, dtype: int64\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Looking at the top 5 values in each column and the number of times they appear\n",
    "unique_values_top_5 = unique_values.head(5)\n",
    "for column in unique_values_top_5['Column']:\n",
    "    # print(f\"\\n\\n{column}\")\n",
    "    print(df[column].value_counts().head())\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5f51cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Currency_Other            99.249324\n",
       "Income_Context            89.159670\n",
       "Job_Title_Add_Comments    74.124804\n",
       "Bonus                     26.024619\n",
       "State_Work_USA            17.909492\n",
       "Highest_Qualification      0.793368\n",
       "Race                       0.629714\n",
       "Gender                     0.608368\n",
       "City_Work                  0.291732\n",
       "Work_Industry              0.266828\n",
       "Job_Title                  0.003558\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the missing values percentage\n",
    "missing_values = df.isnull().mean() * 100\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91aeeb",
   "metadata": {},
   "source": [
    "**Reasons For High Null values**\n",
    "1. Many respondents likely used one of the standard currency options, so they had no need to fill in “Currency_Other.”\n",
    "2. For “Income_Context,” participants may have felt their salary situation was straightforward enough and therefore left that field blank.\n",
    "3. With “Job_Title_Add_Comments,” respondents might not have considered additional comments necessary or found the question optional, resulting in a high rate of non-responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fdb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a848f6e-a0e2-43a2-90fa-76821daed60c",
   "metadata": {},
   "source": [
    "**Observations** <br>\n",
    "Based on the exploratory data analysis, this dataset appears to be in a relatively messy state with several data quality challenges that need to be addressed. Here's a detailed assessment:\n",
    "\n",
    "The dataset contains salary survey responses from over 28,000 participants, with 18 columns covering various aspects of employment information. One of the most significant issues is the inconsistency in data entry formats, particularly evident in columns like Job_Title (14,377 unique values), City_Work (4,841 unique values), and Work_Industry (1,220 unique values). This high number of unique values suggests a lack of standardization in how respondents entered their information, making it difficult to perform meaningful aggregations or analyses without substantial cleaning.\n",
    "\n",
    "The missing value analysis reveals concerning patterns, with some columns having extremely high null rates. Currency_Other (99.25%), Income_Context (89.16%), and Job_Title_Add_Comments (74.12%) show the highest percentages of missing values. While some of these missing values may be legitimate (for example, Currency_Other would naturally be empty for those using standard currencies), others might represent data collection issues or participant non-response that could potentially bias our analysis.\n",
    "\n",
    "The numerical data, particularly in the Annual_Salary and Bonus columns, shows signs of potential data quality issues. The presence of extreme values (maximum salary of 6 billion and maximum bonus of 120 million) suggests either data entry errors or outliers that need careful consideration. Additionally, the salary data was originally stored as strings with commas, requiring conversion to numeric format for analysis. The wide range in these values and the presence of multiple currencies (11 unique currency types) adds another layer of complexity to the data cleaning process.\n",
    "\n",
    "The categorical variables show varying degrees of standardization issues. For example, the Country_Work column contains 382 unique values, far more than the actual number of countries in the world, indicating variations in spelling, capitalization, or format (e.g., \"US\", \"USA\", \"United States\" likely referring to the same country). Similar patterns are observed in the State_Work_USA column (137 unique values) and Race column (51 unique values), where free-form text entry has led to multiple representations of the same categories.\n",
    "\n",
    "The demographic information (age, gender, race) and educational/experience data appear more structured, with reasonable numbers of unique values, though they still contain some missing values that need to be addressed. The timestamp column indicates that all data was collected within a specific timeframe, providing some consistency in the temporal aspect of the dataset.\n",
    "\n",
    "Overall, while the dataset contains valuable information about salary distributions across various demographics and industries, its current state requires substantial cleaning and standardization before it can be effectively used for analysis. The main challenges lie in handling the free-form text entries, addressing missing values, dealing with currency conversions, and managing outliers while preserving the integrity of the underlying data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e3606-db07-41fa-a6b1-f31c3b6ffbde",
   "metadata": {},
   "source": [
    "#### The Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfa01e-9eb1-4dcd-a5c0-a42e4b3b53a9",
   "metadata": {},
   "source": [
    "**Data Standardization and Currency Handling:**\n",
    "First, we need to standardize the salary data by converting all currencies to USD. We'll create a currency conversion mapping using the Currency and Currency_Other columns. We'll also need to clean the Annual_Salary column by removing any non-numeric characters and standardizing number formats. Extreme outliers (like the 6 billion salary) should be investigated individually and corrected if they're clearly errors, rather than being automatically removed.\n",
    "\n",
    "**Geographic Data Cleanup:**\n",
    "The Country_Work column needs standardization to consolidate variations like \"US\", \"USA\", and \"United States\". We can create a mapping dictionary for common variations. Similarly, for State_Work_USA, we'll standardize state names to their official two-letter abbreviations. The City_Work column, while messy, should be standardized for major cities but may need to be left as-is for smaller locations to avoid incorrect mappings. This geographic standardization will be particularly important for salary analysis across regions.\n",
    "\n",
    "**Industry Consolidation:**\n",
    "The Work_Industry column requires categorization into broader industry groups while preserving specific sub-industries in a new column. The Job_Title_Add_Comments column should be preserved as-is since it contains valuable contextual information.\n",
    "\n",
    "**Race Binary Encoding:**\n",
    "For the Race column, we'll create multiple binary columns for each possible race category since it allows multiple selections. For example:\n",
    "- Race_White (0/1)\n",
    "- Race_Asian (0/1)\n",
    "- Race_Black (0/1)\n",
    "- Race_Hispanic (0/1)\n",
    "- Race_Native_American (0/1)\n",
    "- Race_Pacific_Islander (0/1)\n",
    "\n",
    "This approach will allow for proper representation of multi-racial identifications and make it easier to analyze demographic patterns.\n",
    "\n",
    "**Handling Missing Values:**\n",
    "For columns with high missing rates, we'll use different strategies:\n",
    "- Currency_Other (99.25% missing): Can be safely left as-is since it's only relevant for non-standard currencies\n",
    "- Income_Context (89.16% missing): Preserve as supplementary information without imputation\n",
    "- Job_Title_Add_Comments (74.12% missing): Keep as-is since it's supplementary\n",
    "- Bonus (26.02% missing): Replace with 0 for roles where bonuses aren't typical, based on industry patterns\n",
    "- State_Work_USA (17.91% missing): Only fill for confirmed US locations, leave others as missing\n",
    "\n",
    "**Demographic Data Cleanup:**\n",
    "For demographic columns (gender), we'll:\n",
    "- Standardize gender categories while preserving non-binary and other gender identities and null value handing\n",
    "\n",
    "**Experience and Education:**\n",
    "The Total_Years_of_Experience and Related_Work_Experience columns need to be converted to numeric ranges for analysis. We'll create minimum and maximum years columns while preserving the original categorical data. The Highest_Qualification column should be standardized to common degree terminology while maintaining the original granularity.\n",
    "\n",
    "**Outlier Management:**\n",
    "Instead of removing outliers, we'll flag them in new columns:\n",
    "- Salary_Outlier_Flag: Based on industry-specific IQR calculations\n",
    "- Experience_Mismatch_Flag: Where total experience seems inconsistent with age\n",
    "- Bonus_Outlier_Flag: For unusually high bonus-to-salary ratios\n",
    "\n",
    "This approach preserves the original data while maintaining its richness and allowing for different levels of analysis granularity. By keeping Job_Title as-is and implementing binary race columns, we avoid losing valuable information while making the race data more analytically useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca688475-573d-4cd7-b29a-fecf53ff6976",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286a2a7-a6b6-4f77-9e46-1f5e9602bb03",
   "metadata": {},
   "source": [
    "Based on the plan the you described above, go ahead and clean up the dataset.\n",
    "\n",
    "[Add as many code cell below here as needs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2454aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_string(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "        \n",
    "    # Convert to lowercase and remove special characters\n",
    "    cleaned = str(text).lower()\n",
    "    \n",
    "    # Remove everything in parentheses and after common separators\n",
    "    cleaned = re.sub(r'\\(.*?\\)', '', cleaned)\n",
    "    cleaned = re.sub(r'but.*$', '', cleaned)\n",
    "    cleaned = re.sub(r'for.*$', '', cleaned)\n",
    "    cleaned = re.sub(r'based.*$', '', cleaned)\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', cleaned)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    # Return None for non-country entries\n",
    "    if len(cleaned) < 2:  # Too short to be a country\n",
    "        return None\n",
    "        \n",
    "    # Common variations mapping\n",
    "    us_patterns = [\n",
    "        r'^u\\s*s\\s*a*$',  # usa, us, u s a\n",
    "        r'^united\\s*states?.*$',  # united states, united state\n",
    "        r'^us[st][ast].*$',  # ussa, usst, usta, etc.\n",
    "        r'^unit.*stat.*$',  # united states with typos\n",
    "        r'.*states?\\s*of\\s*americ[as]*$'  # states of america variations\n",
    "    ]\n",
    "    \n",
    "    uk_patterns = [\n",
    "        r'^uk$',\n",
    "        r'^united\\s*kingdom.*$',\n",
    "        r'^england.*$',\n",
    "        r'^scotland.*$',\n",
    "        r'^wales.*$',\n",
    "        r'^great\\s*britain.*$',\n",
    "        r'^britain.*$'\n",
    "    ]\n",
    "    \n",
    "    # Check for US variations\n",
    "    for pattern in us_patterns:\n",
    "        if re.match(pattern, cleaned):\n",
    "            return 'United States'\n",
    "            \n",
    "    # Check for UK variations\n",
    "    for pattern in uk_patterns:\n",
    "        if re.match(pattern, cleaned):\n",
    "            return 'United Kingdom'\n",
    "            \n",
    "    # Other common variations\n",
    "    common_fixes = {\n",
    "        'brasil': 'Brazil',\n",
    "        'uae': 'United Arab Emirates',\n",
    "        'hong konh?g?': 'Hong Kong',\n",
    "        'viet\\s*nam': 'Vietnam',\n",
    "        'myanmar': 'Myanmar',\n",
    "        'burma': 'Myanmar',\n",
    "        'czechia': 'Czech Republic',\n",
    "        'ceska republika': 'Czech Republic',\n",
    "        'nederland': 'Netherlands',\n",
    "        'new zealand aotearoa': 'New Zealand',\n",
    "        'canda': 'Canada',\n",
    "        'canadw': 'Canada',\n",
    "        'australi.*': 'Australia'\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in common_fixes.items():\n",
    "        if re.match(pattern, cleaned):\n",
    "            return replacement\n",
    "            \n",
    "    # Remove non-country entries\n",
    "    non_countries = [\n",
    "        'remote', 'global', 'international', 'worldwide', 'europe',\n",
    "        'policy', 'contracts', 'finance', 'bonus', 'salary', 'benefits',\n",
    "        'company', 'government', 'position', 'employee'\n",
    "    ]\n",
    "    \n",
    "    if cleaned in non_countries or any(word in cleaned for word in non_countries):\n",
    "        return None\n",
    "        \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f53cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique country values\n",
    "unique_countries = df['Country_Work'].dropna().unique()\n",
    "\n",
    "# Create mapping dictionary\n",
    "country_mapping = {}\n",
    "for country in unique_countries:\n",
    "    try:\n",
    "        cleaned = clean_string(country)\n",
    "        if cleaned is None:\n",
    "            country_mapping[country] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                found = pycountry.countries.search_fuzzy(cleaned)\n",
    "                country_mapping[country] = found[0].name\n",
    "            except:\n",
    "                if cleaned.title() in [c.name for c in pycountry.countries]:\n",
    "                    country_mapping[country] = cleaned.title()\n",
    "                else:\n",
    "                    country_mapping[country] = np.nan\n",
    "    except:\n",
    "        country_mapping[country] = np.nan\n",
    "\n",
    "# Apply mapping\n",
    "df['Country_Work'] = df['Country_Work'].map(country_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "464c4ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'United States': 23143,\n",
       " 'Canada': 1686,\n",
       " 'United Kingdom': 1582,\n",
       " 'Australia': 391,\n",
       " 'Germany': 197,\n",
       " 'New Zealand': 130,\n",
       " 'Ireland': 125,\n",
       " 'Netherlands': 90,\n",
       " 'France': 68,\n",
       " 'Spain': 62,\n",
       " 'Sweden': 41,\n",
       " 'Switzerland': 38,\n",
       " 'Belgium': 35,\n",
       " 'Japan': 29,\n",
       " 'Denmark': 24,\n",
       " 'India': 23,\n",
       " 'American Samoa': 22,\n",
       " 'South Africa': 20,\n",
       " 'Singapore': 20,\n",
       " 'Austria': 18,\n",
       " 'Finland': 16,\n",
       " 'Italy': 15,\n",
       " 'Norway': 14,\n",
       " 'Israel': 14,\n",
       " 'Malaysia': 13,\n",
       " 'Philippines': 13,\n",
       " 'Brazil': 12,\n",
       " 'China': 11,\n",
       " 'Poland': 10,\n",
       " 'Mexico': 8,\n",
       " 'Virgin Islands, U.S.': 7,\n",
       " 'Czechia': 6,\n",
       " 'Argentina': 6,\n",
       " 'Thailand': 6,\n",
       " 'Greece': 5,\n",
       " 'Colombia': 5,\n",
       " 'Taiwan, Province of China': 5,\n",
       " 'Korea, Republic of': 5,\n",
       " 'Portugal': 5,\n",
       " 'Nigeria': 5,\n",
       " 'Pakistan': 5,\n",
       " 'Romania': 5,\n",
       " 'Hong Kong': 5,\n",
       " 'Puerto Rico': 4,\n",
       " 'Latvia': 4,\n",
       " 'Kenya': 3,\n",
       " 'Ghana': 3,\n",
       " 'Chile': 3,\n",
       " 'Sri Lanka': 2,\n",
       " 'Bermuda': 2,\n",
       " 'Hungary': 2,\n",
       " 'Luxembourg': 2,\n",
       " 'Zimbabwe': 2,\n",
       " 'Namibia': 2,\n",
       " 'Indonesia': 2,\n",
       " 'Lithuania': 2,\n",
       " 'Slovenia': 2,\n",
       " 'Myanmar': 2,\n",
       " 'Cyprus': 2,\n",
       " 'Ukraine': 2,\n",
       " 'Estonia': 2,\n",
       " 'Saudi Arabia': 2,\n",
       " 'Bulgaria': 2,\n",
       " 'Viet Nam': 2,\n",
       " 'Croatia': 2,\n",
       " 'United Arab Emirates': 2,\n",
       " 'Bangladesh': 2,\n",
       " 'Russian Federation': 2,\n",
       " 'Morocco': 2,\n",
       " 'Kuwait': 1,\n",
       " 'Trinidad and Tobago': 1,\n",
       " 'Cuba': 1,\n",
       " 'Sierra Leone': 1,\n",
       " 'Slovakia': 1,\n",
       " 'Somalia': 1,\n",
       " 'Afghanistan': 1,\n",
       " 'Rwanda': 1,\n",
       " 'Cayman Islands': 1,\n",
       " 'Serbia': 1,\n",
       " 'Maldives': 1,\n",
       " 'Iceland': 1,\n",
       " 'Eritrea': 1,\n",
       " 'Cambodia': 1,\n",
       " 'Costa Rica': 1,\n",
       " 'Uganda': 1,\n",
       " 'Syrian Arab Republic': 1,\n",
       " 'Malta': 1,\n",
       " 'Bahamas': 1,\n",
       " 'Qatar': 1,\n",
       " 'Panama': 1,\n",
       " 'Congo': 1,\n",
       " 'Uruguay': 1,\n",
       " 'Jamaica': 1,\n",
       " 'Isle of Man': 1,\n",
       " 'Ecuador': 1,\n",
       " 'Jordan': 1,\n",
       " 'Liechtenstein': 1,\n",
       " 'Bosnia and Herzegovina': 1,\n",
       " 'South Sudan': 1,\n",
       " 'Tanzania, United Republic of': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Country_Work.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5ea733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "city_state_map = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for _, row in df[\n",
    "    (df['Country_Work'] == 'United States') & \n",
    "    (df['State_Work_USA'].notna()) & \n",
    "    (df['City_Work'].notna())\n",
    "].iterrows():\n",
    "    city = row['City_Work'].strip().lower()\n",
    "    state = row['State_Work_USA'].strip()\n",
    "    city_state_map[city][state] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4520955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boston': 'Massachusetts',\n",
       " 'chattanooga': 'Tennessee',\n",
       " 'milwaukee': 'Wisconsin',\n",
       " 'greenville': 'South Carolina',\n",
       " 'hanover': 'New Hampshire',\n",
       " 'columbia': 'South Carolina',\n",
       " 'yuma': 'Arizona',\n",
       " 'st. louis': 'Missouri',\n",
       " 'palm coast': 'Florida',\n",
       " 'scranton': 'Pennsylvania',\n",
       " 'detroit': 'Michigan',\n",
       " 'saint paul': 'Minnesota',\n",
       " 'chicago': 'Illinois',\n",
       " 'pomona': 'California',\n",
       " 'atlanta': 'Georgia',\n",
       " 'boca raton': 'Florida',\n",
       " 'philadelphia': 'Pennsylvania',\n",
       " 'dayton': 'Ohio',\n",
       " 'bradenton': 'Florida',\n",
       " 'ann arbor': 'Michigan',\n",
       " 'washington dc': 'District of Columbia',\n",
       " 'silver spring': 'Maryland',\n",
       " 'washington': 'District of Columbia',\n",
       " 'san antonio': 'Texas',\n",
       " 'minneapolis': 'Minnesota',\n",
       " 'washington, dc': 'District of Columbia',\n",
       " 'richmond': 'Virginia',\n",
       " 'research triangle': 'North Carolina',\n",
       " 'kalamazoo': 'Michigan',\n",
       " 'manhattan': 'New York',\n",
       " 'sacramento': 'California',\n",
       " 'dallas': 'Texas',\n",
       " 'waynesboro': 'Virginia',\n",
       " 'pittsburgh': 'Pennsylvania',\n",
       " 'arlington, va': 'Virginia',\n",
       " 'chapel hill': 'North Carolina',\n",
       " 'berkeley': 'California',\n",
       " 'apple valley': 'Minnesota',\n",
       " 'troy': 'Michigan',\n",
       " 'dc': 'District of Columbia',\n",
       " 'linden': 'New Jersey',\n",
       " 'champaign': 'Illinois',\n",
       " 'bryan': 'Texas',\n",
       " 'providence': 'Rhode Island',\n",
       " 'district of columbia': 'District of Columbia',\n",
       " 'bloomington': 'Indiana',\n",
       " 'denver': 'Colorado',\n",
       " 'new york city': 'New York',\n",
       " 'prefer not to answer': 'Maryland',\n",
       " 'nyc (remotely)': 'New York',\n",
       " 'raleigh': 'North Carolina',\n",
       " 'new york': 'New York',\n",
       " '-----': 'Pennsylvania',\n",
       " 'st. paul': 'Minnesota',\n",
       " 'philadelphia (suburbs)': 'Pennsylvania',\n",
       " 'portland': 'Oregon',\n",
       " 'nashville': 'Tennessee',\n",
       " 'seattle': 'Washington',\n",
       " 'indianapolis': 'Indiana',\n",
       " 'baltimore': 'Maryland',\n",
       " 'solon': 'Ohio',\n",
       " 'gainesville': 'Florida',\n",
       " 'lafayette': 'Louisiana',\n",
       " 'phoenix': 'Arizona',\n",
       " 'boulder': 'Colorado',\n",
       " 'austin': 'Texas',\n",
       " 'portsmouth': 'New Hampshire',\n",
       " 'kansas city': 'Missouri',\n",
       " 'dubuque': 'Iowa',\n",
       " 'san diego': 'California',\n",
       " 'belmont': 'Massachusetts',\n",
       " 'cambridge': 'Massachusetts',\n",
       " 'whippany': 'New Jersey',\n",
       " 'jefferson': 'Wisconsin',\n",
       " 'manassas': 'Virginia',\n",
       " 'detroit metro area': 'Michigan',\n",
       " 'newtown square': 'Pennsylvania',\n",
       " 'small city, remote, national company': 'Florida',\n",
       " 'memphis': 'Tennessee',\n",
       " 'grand rapids': 'Michigan',\n",
       " 'omaha': 'Nebraska',\n",
       " 'mclean': 'Virginia',\n",
       " 'ardmore': 'Oklahoma',\n",
       " 'columbus': 'Ohio',\n",
       " 'shaker hts': 'Ohio',\n",
       " 'charleston': 'South Carolina',\n",
       " 'madison': 'Wisconsin',\n",
       " 'mishawaka': 'Indiana',\n",
       " 'tulsa': 'Oklahoma',\n",
       " 'tampa': 'Florida',\n",
       " 'lambertville': 'New Jersey',\n",
       " 'chatham': 'New York',\n",
       " 'skokie': 'Illinois',\n",
       " 'alpharetta': 'Georgia',\n",
       " 'northeast florida': 'Florida',\n",
       " 'altamonte springs': 'Florida',\n",
       " 'san francisco': 'California',\n",
       " 'el paso': 'Texas',\n",
       " 'research triangle region': 'North Carolina',\n",
       " 'merced': 'California',\n",
       " 'gorham/portland': 'Maine',\n",
       " 'westchester/nyc': 'New York',\n",
       " 'san jose': 'California',\n",
       " 'grandville': 'Michigan',\n",
       " 'houston': 'Texas',\n",
       " 'eau claire': 'Wisconsin',\n",
       " 'buffalo': 'New York',\n",
       " 'west lafayette': 'Indiana',\n",
       " 'too identifiable': 'Georgia',\n",
       " 'braintree': 'Massachusetts',\n",
       " 'clinton': 'Indiana',\n",
       " 'woburn': 'Massachusetts',\n",
       " 'anaheim': 'California',\n",
       " 'orlando': 'Florida',\n",
       " 'appleton': 'Wisconsin',\n",
       " 'jacksonville': 'Florida',\n",
       " 'durham': 'North Carolina',\n",
       " 'stamford': 'Connecticut',\n",
       " 'newport': 'Rhode Island',\n",
       " 'los angeles': 'California',\n",
       " 'new brunswick': 'New Jersey',\n",
       " 'chicago area mostly, but also the us and canada': 'Illinois',\n",
       " 'spokane': 'Washington',\n",
       " 'greater boston area': 'Massachusetts',\n",
       " 'chicago (remote)': 'Illinois',\n",
       " 'rapid city': 'South Dakota',\n",
       " 'mount prospect': 'Illinois',\n",
       " 'huntington (remote, hq is in charleston)': 'West Virginia',\n",
       " 'remote': 'New York',\n",
       " 'charlotte': 'North Carolina',\n",
       " 'flagstaff': 'Arizona',\n",
       " 'carlinville': 'Illinois',\n",
       " 'chaska': 'Minnesota',\n",
       " 'western mass': 'Massachusetts',\n",
       " 'joliet': 'Illinois',\n",
       " 'shingle springs': 'California',\n",
       " 'plant city': 'Florida',\n",
       " 'nyc': 'New York',\n",
       " 'lisle': 'Illinois',\n",
       " 'salem': 'Oregon',\n",
       " 'platteville': 'Wisconsin',\n",
       " 'suitland': 'Maryland',\n",
       " 'rockville': 'Maryland',\n",
       " 'north east': 'Pennsylvania',\n",
       " 'houston area': 'Texas',\n",
       " 'charlottesville': 'Virginia',\n",
       " 'idaho falls': 'Idaho',\n",
       " 'rochester': 'New York',\n",
       " 'metro boston': 'Massachusetts',\n",
       " 'twin cities metro area': 'Minnesota',\n",
       " 'duluth': 'Minnesota',\n",
       " 'south hadley': 'Massachusetts',\n",
       " 'dfw area': 'Texas',\n",
       " 'north andover': 'Massachusetts',\n",
       " 'bellevue': 'Washington',\n",
       " 'suwanee': 'Georgia',\n",
       " 'fairfield': 'Connecticut',\n",
       " 'manchester': 'New Hampshire',\n",
       " 'dfw': 'Texas',\n",
       " 'wilmington': 'Delaware',\n",
       " 'cherry hill': 'New Jersey',\n",
       " 'cleveland': 'Ohio',\n",
       " 'fully remote (greater boston)': 'Massachusetts',\n",
       " 'east hanover': 'New Jersey',\n",
       " 'new orleans': 'Louisiana',\n",
       " 'citrus heights': 'California',\n",
       " 'jefferson city': 'Missouri',\n",
       " 'naples': 'Florida',\n",
       " 'melbourne': 'Florida',\n",
       " 'medina': 'Ohio',\n",
       " 'prefer not to say (montana is small!)': 'Montana',\n",
       " 'lexington': 'Kentucky',\n",
       " 'la crosse': 'Wisconsin',\n",
       " 'springfield': 'Massachusetts',\n",
       " 'canton': 'Ohio',\n",
       " 'richardson': 'Texas',\n",
       " 'panhandle region': 'Florida',\n",
       " 'glendale': 'Arizona',\n",
       " 'golden': 'Colorado',\n",
       " 'bay area': 'California',\n",
       " 'des moines': 'Iowa',\n",
       " 'college park': 'Maryland',\n",
       " 'redding': 'California',\n",
       " 'i work remotely': 'New York',\n",
       " 'springfield, il': 'Illinois',\n",
       " 'columbia, sc': 'South Carolina',\n",
       " 'san francisco bay area': 'California',\n",
       " 'remote/ home': 'New York',\n",
       " 'harrisonburg, va': 'Virginia',\n",
       " 'somerset': 'New Jersey',\n",
       " 'normal': 'Illinois',\n",
       " 'eglin afb': 'Florida',\n",
       " 'work from home': 'Massachusetts',\n",
       " 'grand forks': 'North Dakota',\n",
       " 'bloomfield hills': 'Michigan',\n",
       " 'pembroke': 'Virginia',\n",
       " 'redmond': 'Washington',\n",
       " 'annandale': 'Virginia',\n",
       " 'prineville': 'Oregon',\n",
       " 'arlington': 'Virginia',\n",
       " 'overland park': 'Kansas',\n",
       " 'oakland': 'California',\n",
       " 'rutland': 'Vermont',\n",
       " 'ames': 'Iowa',\n",
       " 'verysmall town': 'Iowa',\n",
       " 'rockland': 'Maine',\n",
       " 'concord': 'New Hampshire',\n",
       " 'south bend': 'Indiana',\n",
       " 'siloam springs': 'Arkansas',\n",
       " 'irvine': 'California',\n",
       " 'princeton': 'New Jersey',\n",
       " 'twin cities suburbs': 'Minnesota',\n",
       " 'cincinnati': 'Ohio',\n",
       " 'montgomery': 'Alabama',\n",
       " 'leesburg': 'Virginia',\n",
       " 'carpinteria': 'California',\n",
       " 'albany': 'New York',\n",
       " 'la': 'California',\n",
       " 'ridgecrest': 'California',\n",
       " 'everett': 'Washington',\n",
       " 'miami': 'Florida',\n",
       " 'east stroudsburg': 'Pennsylvania',\n",
       " 'st louis': 'Missouri',\n",
       " 'twin falls': 'Idaho',\n",
       " 'lake mills': 'Wisconsin',\n",
       " 'ripon': 'California',\n",
       " 'lansing': 'Michigan',\n",
       " 'las vegas': 'Nevada',\n",
       " 'las vegas (techinically remote)': 'Nevada',\n",
       " 'oak park': 'Illinois',\n",
       " 'ironwood': 'Michigan',\n",
       " 'fairfax': 'Virginia',\n",
       " 'huntsville': 'Alabama',\n",
       " 'exton': 'Pennsylvania',\n",
       " 'fort smith': 'Arkansas',\n",
       " 'lake forest': 'Illinois',\n",
       " 'bethesda': 'Maryland',\n",
       " 'tallahassee, fl': 'Florida',\n",
       " 'east hampton': 'New York',\n",
       " 'washington, dc.': 'District of Columbia',\n",
       " 'camp hill': 'Pennsylvania',\n",
       " 'lawrence': 'Kansas',\n",
       " 'bothell': 'Washington',\n",
       " \"philly 'burbs\": 'New Jersey',\n",
       " 'lebanon': 'New Hampshire',\n",
       " 'newark': 'New Jersey',\n",
       " 'bethlehem': 'Pennsylvania',\n",
       " 'lake zurich': 'Illinois',\n",
       " 'flemington': 'New Jersey',\n",
       " 'this is too identifying': 'Colorado',\n",
       " 'noble': 'Oklahoma',\n",
       " 'east hartford': 'Connecticut',\n",
       " 'middleton': 'Wisconsin',\n",
       " 'maple grove': 'Minnesota',\n",
       " 'atlanta metro area': 'Georgia',\n",
       " 'alexandria': 'Virginia',\n",
       " 'aberdeen': 'Washington',\n",
       " 'louisville': 'Kentucky',\n",
       " 'newport beach': 'California',\n",
       " 'south texas': 'Texas',\n",
       " 'kirtland': 'New Mexico',\n",
       " 'bellingham': 'Washington',\n",
       " 'new haven': 'Connecticut',\n",
       " 'greater new orleans metro region': 'Louisiana',\n",
       " 'lowell': 'Massachusetts',\n",
       " 'niles': 'Michigan',\n",
       " 'honolulu': 'Hawaii',\n",
       " 'dearborn': 'Michigan',\n",
       " 'would rather not say': 'Michigan',\n",
       " 'augusta': 'Maine',\n",
       " 'schenectady': 'New York',\n",
       " 'washington, d.c.': 'District of Columbia',\n",
       " 'salt lake city': 'Utah',\n",
       " 'las cruces': 'New Mexico',\n",
       " 'fayetteville': 'Arkansas',\n",
       " 'trenton': 'New Jersey',\n",
       " 'gaithersburg': 'Maryland',\n",
       " 'annapolis': 'Maryland',\n",
       " 'broomfield': 'Colorado',\n",
       " 'mount laurel': 'New Jersey',\n",
       " 'newton': 'Massachusetts',\n",
       " 'new castle': 'Delaware',\n",
       " 'topeka': 'Kansas',\n",
       " 'brooklyn': 'New York',\n",
       " 'prefer not to say': 'Michigan',\n",
       " 'somerville': 'Massachusetts',\n",
       " 'auburn hills': 'Michigan',\n",
       " 'moline': 'Illinois',\n",
       " 'spartanburg': 'South Carolina',\n",
       " 'athens': 'Georgia',\n",
       " 'new ulm': 'Minnesota',\n",
       " 'waco': 'Texas',\n",
       " 'boise': 'Idaho',\n",
       " 'falls church': 'Virginia',\n",
       " 'truckee': 'California',\n",
       " 'durham area': 'North Carolina',\n",
       " 'plano': 'Texas',\n",
       " 'bedford': 'New Hampshire',\n",
       " 'rochester hills': 'Michigan',\n",
       " 'reno': 'Nevada',\n",
       " 'erie': 'Pennsylvania',\n",
       " 'dover': 'Delaware',\n",
       " 'fort wayne': 'Indiana',\n",
       " 'bristol': 'Virginia',\n",
       " 'central kentucky': 'Kentucky',\n",
       " 'terrell': 'Texas',\n",
       " 'lehi': 'Utah',\n",
       " 'full time remote from my home near a major pa city': 'Pennsylvania',\n",
       " 'st paul, mn': 'Minnesota',\n",
       " 'saint louis': 'Missouri',\n",
       " 'greensboro': 'North Carolina',\n",
       " 'northfield': 'Minnesota',\n",
       " 'hartford': 'Connecticut',\n",
       " 'pocatello': 'Idaho',\n",
       " 'naval base': 'California',\n",
       " 'dallas/fort worth region': 'Texas',\n",
       " 'winston-salem': 'North Carolina',\n",
       " 'fort collins': 'Colorado',\n",
       " 'merritt island': 'Florida',\n",
       " 'philadelphia suburbs': 'Pennsylvania',\n",
       " 'suburb of chicago': 'Illinois',\n",
       " 'ny metro': 'New Jersey',\n",
       " 'medford': 'Oregon',\n",
       " 'norfolk': 'Virginia',\n",
       " 'rolla': 'Missouri',\n",
       " 'prefer not to disclose': 'Pennsylvania',\n",
       " 'mountain view': 'California',\n",
       " 'syracuse': 'New York',\n",
       " 'southfield': 'Michigan',\n",
       " 'westerly': 'Rhode Island',\n",
       " 'orange': 'California',\n",
       " 'lincoln': 'Nebraska',\n",
       " 'tallahassee': 'Florida',\n",
       " 'sioux falls': 'South Dakota',\n",
       " 'wichita': 'Kansas',\n",
       " 'san jose, ca': 'California',\n",
       " 'suburban chicago': 'Illinois',\n",
       " 'decherd': 'Tennessee',\n",
       " 'san rafael': 'California',\n",
       " 'franklin': 'Wisconsin',\n",
       " 'sunnyvale': 'California',\n",
       " 'greater philadelphia area': 'Pennsylvania',\n",
       " 'denver metro area': 'Colorado',\n",
       " 'chester': 'Pennsylvania',\n",
       " 'plattsburgh, ny': 'New York',\n",
       " 'tigard': 'Oregon',\n",
       " 'center': 'Colorado',\n",
       " 'beaufort': 'South Carolina',\n",
       " 'baltimore suburbs': 'Maryland',\n",
       " 'allentown': 'Pennsylvania',\n",
       " 'albuquerque': 'New Mexico',\n",
       " 'greenbelt': 'Maryland',\n",
       " 'minnespolis': 'Minnesota',\n",
       " 'roanoke': 'Virginia',\n",
       " 'poughkeepsie': 'New York',\n",
       " 'greater chicago': 'Illinois',\n",
       " 'albany-schenectady-troy metro area': 'New York',\n",
       " 'northwest georgia': 'Georgia',\n",
       " 'plymouth': 'Minnesota',\n",
       " 'evanston': 'Illinois',\n",
       " 'knoxville': 'Tennessee',\n",
       " 'cary': 'North Carolina',\n",
       " 'colorado springs': 'Colorado',\n",
       " 'none': 'Michigan',\n",
       " 'delray beach': 'Florida',\n",
       " 'la vergne': 'Tennessee',\n",
       " 'witchita': 'Kansas',\n",
       " 'semirural city': 'Virginia',\n",
       " 'burlington': 'Vermont',\n",
       " 'pensacola': 'Florida',\n",
       " 'herndon': 'Virginia',\n",
       " 'brookline': 'Massachusetts',\n",
       " 'frisco': 'Texas',\n",
       " 'east moline': 'Illinois',\n",
       " 'utah county': 'Utah',\n",
       " 'titusville': 'Florida',\n",
       " 'eastern ma (not boston)': 'Massachusetts',\n",
       " 'bridgewater': 'New Jersey',\n",
       " 'conshohocken': 'Pennsylvania',\n",
       " 'rural': 'Pennsylvania',\n",
       " 'torrington': 'Massachusetts',\n",
       " 'west palm beaxh': 'Florida',\n",
       " 'carson city': 'Nevada',\n",
       " 'salisbury': 'Maryland',\n",
       " 'crystal city': 'Virginia',\n",
       " 'st. louis park': 'Minnesota',\n",
       " 'santa fe': 'New Mexico',\n",
       " 'marion': 'Iowa',\n",
       " 'fully remote job (denver area)': 'Colorado',\n",
       " 'amherst': 'Massachusetts',\n",
       " 'vancouver': 'Washington',\n",
       " 'cincinatti': 'Ohio',\n",
       " 'woodinville': 'Washington',\n",
       " 'pasadena': 'California',\n",
       " 'calumet city': 'Illinois',\n",
       " 'fortville': 'Indiana',\n",
       " 'remote worker': 'Oregon',\n",
       " 'peoria': 'Illinois',\n",
       " 'lyndon': 'Kansas',\n",
       " 'worcester': 'Massachusetts',\n",
       " 'drop down menur was not working. indianapolis indiana.': 'Kentucky',\n",
       " 'swiftwater': 'Pennsylvania',\n",
       " 'chesterbrook': 'Pennsylvania',\n",
       " 'paris': 'Texas',\n",
       " 'mystic': 'Connecticut',\n",
       " 'north port': 'Florida',\n",
       " 'washington d.c.': 'District of Columbia',\n",
       " 'wake forest': 'North Carolina',\n",
       " 'philly': 'Pennsylvania',\n",
       " 'kingston': 'New York',\n",
       " 'chicago suburbs': 'Illinois',\n",
       " 'sarasota': 'Florida',\n",
       " 'rural college town': 'Illinois',\n",
       " 'long island (remote)': 'New York',\n",
       " 'rural florida panhandle': 'Illinois',\n",
       " 'durham, nc': 'North Carolina',\n",
       " 'metro atlanta': 'Georgia',\n",
       " 'portland, or': 'Oregon',\n",
       " 'morristown': 'New Jersey',\n",
       " 'san deigo': 'California',\n",
       " 'full time remote employee (prior to pandemic)': 'Arizona',\n",
       " 'beaverton': 'Oregon',\n",
       " 'elkton': 'Maryland',\n",
       " 'puget sound region': 'Washington',\n",
       " 'alvarado': 'Texas',\n",
       " 'no': 'Indiana',\n",
       " 'hillsborough': 'New Jersey',\n",
       " 'lancaster': 'Pennsylvania',\n",
       " 'mascoutah': 'Illinois',\n",
       " 'georgetown': 'Massachusetts',\n",
       " 'stratham': 'New Hampshire',\n",
       " 'hilliard': 'Ohio',\n",
       " 'pittsburgh, pa': 'Pennsylvania',\n",
       " 'portage': 'Michigan',\n",
       " 'billings': 'Montana',\n",
       " 'chandler': 'Arizona',\n",
       " 'na (remote). live near boston, work is based in upstate ny': 'Massachusetts',\n",
       " 'woodstock': 'Georgia',\n",
       " 'detroit, mi': 'Michigan',\n",
       " 'boston suburbs': 'Massachusetts',\n",
       " 'larkspur': 'California',\n",
       " 'n/a': 'New Hampshire',\n",
       " 'bowling green': 'Kentucky',\n",
       " 'muskego': 'Wisconsin',\n",
       " 'amarillo': 'Texas',\n",
       " 'narragansett': 'Rhode Island',\n",
       " 'fresno': 'California',\n",
       " 'norcross': 'North Carolina',\n",
       " 'east lansing': 'Michigan',\n",
       " 'apache junction': 'Arizona',\n",
       " 'remote but nyc': 'New York',\n",
       " 'white plains': 'New York',\n",
       " 'monterey': 'California',\n",
       " 'western slope': 'Colorado',\n",
       " 'easton': 'Pennsylvania',\n",
       " 'primary wfh or nyc; however travel to client site weekly in the \"before times\"': 'New Jersey',\n",
       " 'richland': 'Washington',\n",
       " 'northern michigan': 'Michigan',\n",
       " 'paducah': 'Kentucky',\n",
       " 'chanhassen': 'Minnesota',\n",
       " 'la mirada': 'California',\n",
       " 'fredericksburg': 'Virginia',\n",
       " 'ft. mitchell': 'Kentucky',\n",
       " 'ringwood': 'New Jersey',\n",
       " 'chicago but the company is remote': 'Illinois',\n",
       " 'riverside': 'California',\n",
       " 'orlando metro area': 'Florida',\n",
       " 'windsor locks': 'Connecticut',\n",
       " 'frederick': 'Maryland',\n",
       " 'this info will make me identifiable': 'Ohio',\n",
       " 'hopkinton': 'Massachusetts',\n",
       " \"can't share\": 'South Carolina',\n",
       " 'dalton': 'Georgia',\n",
       " 'tacoma': 'Washington',\n",
       " 'missoula': 'Montana',\n",
       " 'new bruswick': 'New Jersey',\n",
       " 'baton rouge': 'Louisiana',\n",
       " 'bremerton': 'Washington',\n",
       " 'ypsilanti': 'Michigan',\n",
       " 'santa clara': 'California',\n",
       " 'santa rosa': 'California',\n",
       " 'wilkes barre': 'Pennsylvania',\n",
       " 'hanover area': 'New Hampshire',\n",
       " 'west des moines': 'Iowa',\n",
       " 'maumee': 'Ohio',\n",
       " 'brunswick': 'Maine',\n",
       " 'st. paul/minniapolis area': 'Wisconsin',\n",
       " 'long island': 'New York',\n",
       " 'metairie': 'Louisiana',\n",
       " 'framingham, ma': 'Massachusetts',\n",
       " 'montpelier': 'Vermont',\n",
       " 'blue bell': 'Pennsylvania',\n",
       " 'garner': 'North Carolina',\n",
       " 'oklahoma city': 'Oklahoma',\n",
       " 'ithaca': 'New York',\n",
       " 'boron': 'California',\n",
       " 'na': 'Texas',\n",
       " 'tucson': 'Arizona',\n",
       " 'chester springs, pa': 'Pennsylvania',\n",
       " 'small city': 'Wisconsin',\n",
       " 'iowa city (coralville)': 'Iowa',\n",
       " 'st petersburg': 'Florida',\n",
       " 'savannah': 'Georgia',\n",
       " 'maricopa county': 'Arizona',\n",
       " 'warren': 'Michigan',\n",
       " 'palm springs': 'California',\n",
       " 'fall river': 'Massachusetts',\n",
       " 'maryland': 'Maryland',\n",
       " 'small town': 'Vermont',\n",
       " 'hyannis': 'Massachusetts',\n",
       " 'norman': 'Oklahoma',\n",
       " 'whitewater': 'Wisconsin',\n",
       " 'dublin': 'Ohio',\n",
       " 'vienna': 'Virginia',\n",
       " 'rural area, remote worker': 'New York',\n",
       " 'st paul': 'Minnesota',\n",
       " 'ambler': 'Pennsylvania',\n",
       " 'jackson': 'Mississippi',\n",
       " 'job is based in dc, but i go to where disasters are': 'District of Columbia',\n",
       " 'jonesboro': 'Arkansas',\n",
       " 'nashua': 'New Hampshire',\n",
       " 'hershey': 'Pennsylvania',\n",
       " 'cranbury': 'New Jersey',\n",
       " 'aliso viejo': 'California',\n",
       " 'williamsburg': 'Virginia',\n",
       " 'shoreline': 'Washington',\n",
       " 'horsham': 'Pennsylvania',\n",
       " 'valdosta': 'Georgia',\n",
       " 'montvale': 'New Jersey',\n",
       " 'southampton': 'New York',\n",
       " 'staunton': 'Virginia',\n",
       " 'daytona beach': 'Florida',\n",
       " 'englewood': 'Colorado',\n",
       " 'n/a - county': 'Virginia',\n",
       " 'rural - northern': 'Minnesota',\n",
       " 'central maine': 'Maine',\n",
       " 'ashland': 'Oregon',\n",
       " 'eugene': 'Oregon',\n",
       " 'upstate ny': 'New York',\n",
       " 'irving': 'Texas',\n",
       " 'boston area': 'Massachusetts',\n",
       " 'jamestown': 'New York',\n",
       " 'frankfort': 'Kentucky',\n",
       " 'maquoketa': 'Iowa',\n",
       " 'metro detroit': 'Michigan',\n",
       " 'white river junction, vt': 'Vermont',\n",
       " 'norwalk': 'Connecticut',\n",
       " 'orange, ma': 'Massachusetts',\n",
       " 'doylestown': 'Pennsylvania',\n",
       " 'aldie, va': 'Virginia',\n",
       " 'minnetonka': 'Minnesota',\n",
       " '-': 'North Carolina',\n",
       " 'waltham': 'Massachusetts',\n",
       " 'chantilly': 'Virginia',\n",
       " 'virginia': 'Minnesota',\n",
       " 'andover': 'Massachusetts',\n",
       " 'atlantic city': 'New Jersey',\n",
       " 'boston massachusetts': 'Massachusetts',\n",
       " 'aurora': 'Colorado',\n",
       " 'redwood city': 'California',\n",
       " 'abington, pa': 'Pennsylvania',\n",
       " 'south san francisco': 'California',\n",
       " 'glens falls': 'New York',\n",
       " 'sandpoint': 'Idaho',\n",
       " 'islip': 'New York',\n",
       " 'morgantown': 'West Virginia',\n",
       " 'sf bay area': 'California',\n",
       " 'torrance': 'California',\n",
       " 'oceanside': 'California',\n",
       " 'virgnia beach': 'Virginia',\n",
       " 'akron': 'Ohio',\n",
       " 'cambridge, ma': 'Massachusetts',\n",
       " 'alexandria va': 'Virginia',\n",
       " 'ridgefield': 'Washington',\n",
       " 'largo': 'Maryland',\n",
       " 'cambridge ,ma': 'Massachusetts',\n",
       " 'moscow': 'Idaho',\n",
       " 'menomonee falls': 'Wisconsin',\n",
       " 'johnston': 'Rhode Island',\n",
       " 'denver (but remote - company is nyc)': 'Colorado',\n",
       " 'payroll is out of cincinnati': 'Kentucky, Ohio',\n",
       " 'raleigh nc': 'North Carolina',\n",
       " 'lincoln, ne': 'Nebraska',\n",
       " 'fullerton': 'California',\n",
       " 'hudson, ny': 'New York',\n",
       " 'starkville': 'Mississippi',\n",
       " 'asheville': 'North Carolina',\n",
       " 'olney': 'Maryland',\n",
       " 'tyler': 'Texas',\n",
       " 'new kensington': 'Pennsylvania',\n",
       " 'rochester, ny': 'New York',\n",
       " 'terre haute': 'Indiana',\n",
       " 'toledo': 'Ohio',\n",
       " 'roselle': 'Illinois',\n",
       " 'milwaukee area': 'Wisconsin',\n",
       " 'basking ridge': 'New Jersey',\n",
       " 'new york, ny': 'New York',\n",
       " 'hickory': 'North Carolina',\n",
       " 'my city + industry would id my employer': 'Iowa',\n",
       " 'cape girardeau': 'Missouri',\n",
       " 'romney': 'West Virginia',\n",
       " 'not identifying, but the central valley, ca': 'California',\n",
       " 'acton, ma': 'Massachusetts',\n",
       " 'eufaula': 'Alabama',\n",
       " 'rural small town western ny': 'New York',\n",
       " 'minneapolis suburbs': 'Minnesota',\n",
       " 'ft. lauderdale': 'Florida',\n",
       " 'amory': 'Mississippi',\n",
       " 'philedelphia': 'Pennsylvania',\n",
       " 'cedar rapids': 'Iowa',\n",
       " 'olympia': 'Washington',\n",
       " 'wakefield': 'Massachusetts',\n",
       " 'schaumburg': 'Illinois',\n",
       " 'modesto': 'California',\n",
       " 'scottsdale': 'Arizona',\n",
       " 'oxnard': 'California',\n",
       " 'rather not answer': 'New Jersey',\n",
       " 'chantilly, va': 'Virginia',\n",
       " 'greater nashville area': 'Tennessee',\n",
       " 'albuquerqur': 'New Mexico',\n",
       " 'bonita springs': 'Florida',\n",
       " 'this identifies my employer': 'New Hampshire',\n",
       " 'baltimore county': 'Maryland',\n",
       " 'altus': 'Oklahoma',\n",
       " 'harrisonburg': 'Virginia',\n",
       " 'boston, ma': 'Massachusetts',\n",
       " 'holland': 'Michigan',\n",
       " 'durango': 'Colorado',\n",
       " 'decline to mention': 'Virginia',\n",
       " 'waterloo': 'Iowa',\n",
       " 'salem, ma': 'Massachusetts',\n",
       " 'pascagoula, ms': 'Mississippi',\n",
       " 'farmington hills': 'Michigan',\n",
       " 'costa mesa': 'California',\n",
       " 'parsippany': 'New Jersey',\n",
       " 'bentonville': 'Arkansas',\n",
       " 'quincy': 'Massachusetts',\n",
       " 'currently remote': 'Pennsylvania',\n",
       " 'chicago area': 'Illinois',\n",
       " 'western ks': 'Kansas',\n",
       " 'brewster': 'New York',\n",
       " 'lake tahoe': 'California',\n",
       " 'south jordan': 'Utah',\n",
       " 'los a6': 'California',\n",
       " 'kennebunk': 'Maine',\n",
       " 'shelton': 'Washington',\n",
       " 'york': 'Pennsylvania',\n",
       " 'winston salem': 'North Carolina',\n",
       " 'white oak': 'Maryland',\n",
       " 'onley': 'Virginia',\n",
       " 'corinth': 'Mississippi',\n",
       " 'little chute': 'Wisconsin',\n",
       " 'miami beach': 'Florida',\n",
       " 'longview': 'Texas',\n",
       " 'taylorsville': 'Utah',\n",
       " 'little rock': 'Arkansas',\n",
       " 'purchase': 'New York',\n",
       " 'corvallis': 'Oregon',\n",
       " 'chicago (suburbs)': 'Illinois',\n",
       " 'elizabethtown': 'Kentucky',\n",
       " 'waterford': 'New York',\n",
       " 'bismarck': 'North Dakota',\n",
       " 'watertown': 'Massachusetts',\n",
       " 'potland': 'Oregon',\n",
       " 'lansdale': 'Pennsylvania',\n",
       " 'edison': 'New Jersey',\n",
       " 'asheville, nc': 'North Carolina',\n",
       " 'jersey city': 'New Jersey',\n",
       " 'thousand oaks': 'California',\n",
       " 'lakewood': 'Colorado',\n",
       " 'los alamos': 'New Mexico',\n",
       " 'wayne, pa': 'Pennsylvania',\n",
       " 'houston-galveston-brazoria area': 'Texas',\n",
       " 'portland area': 'Oregon',\n",
       " 'hillsboro': 'Oregon',\n",
       " 'southern il': 'Illinois',\n",
       " 'twin cities': 'Minnesota',\n",
       " 'northern front range': 'Colorado',\n",
       " 'alton': 'Illinois',\n",
       " 'big bear city': 'California',\n",
       " \"company is in nyc, i'm remote in boston\": 'New York',\n",
       " 'dripping springs': 'Texas',\n",
       " 'vicksburg': 'Mississippi',\n",
       " 'birmingham': 'Alabama',\n",
       " 'seattle, wa': 'Washington',\n",
       " 'omaha, ne': 'Nebraska',\n",
       " 'anchorage': 'Alaska',\n",
       " 'boston(ish)': 'Massachusetts',\n",
       " 'los angeles metro': 'California',\n",
       " 'tysons': 'Virginia',\n",
       " 'saginaw': 'Michigan',\n",
       " 'suburban nyc metro area': 'New York',\n",
       " 'kent': 'Washington',\n",
       " 'peabody, ma': 'Massachusetts',\n",
       " 'northville': 'Michigan',\n",
       " 'olympic peninsula': 'Washington',\n",
       " 'worcester area': 'Massachusetts',\n",
       " 'northampton': 'Massachusetts',\n",
       " 'west palm beach': 'Florida',\n",
       " 'reidsville': 'Georgia',\n",
       " 'rockford': 'Illinois',\n",
       " 'downers grove': 'Illinois',\n",
       " 'an exact answer would out me': 'Illinois',\n",
       " 'harrisburg': 'Pennsylvania',\n",
       " 'collegeville': 'Pennsylvania',\n",
       " 'mankato': 'Minnesota',\n",
       " 'prescott': 'Arizona',\n",
       " 'stratford': 'Connecticut',\n",
       " 'marlborough': 'Massachusetts',\n",
       " 'santa cruz': 'California',\n",
       " 'anonymous': 'Florida',\n",
       " 'middletown': 'Connecticut',\n",
       " 'detroit metro': 'Michigan',\n",
       " 'reston': 'Virginia',\n",
       " 'weehawken': 'New Jersey',\n",
       " 'warwick, ri': 'Rhode Island',\n",
       " 'walnut creek': 'California',\n",
       " 'portland, or (remote for a company in ca)': 'California, Oregon',\n",
       " 'fort worth': 'Texas',\n",
       " 'east liverpool': 'Ohio',\n",
       " 'florence': 'Kentucky',\n",
       " 'queensbury': 'New York',\n",
       " 'stafford': 'Virginia',\n",
       " 'baltimore, md': 'Maryland',\n",
       " 'arnold': 'Maryland',\n",
       " 'sabetha': 'Kansas',\n",
       " 'san ramon': 'California',\n",
       " 'san carlos, ca': 'California',\n",
       " 'new city, ny': 'New York',\n",
       " 'somers point': 'New Jersey',\n",
       " 'sacramento, ca': 'California',\n",
       " 'tempe': 'Arizona',\n",
       " 'a very large city': 'Texas',\n",
       " 'milford, massachusetts': 'Massachusetts',\n",
       " 'butler': 'Pennsylvania',\n",
       " 'egg harbor twp': 'New Jersey',\n",
       " 'henrico, va': 'Virginia',\n",
       " 'olathe': 'Kansas',\n",
       " 'natick': 'Massachusetts',\n",
       " 'collingswood': 'New Jersey',\n",
       " 'tennessee': 'Tennessee',\n",
       " 'baltimore suburb': 'Maryland',\n",
       " 'flint': 'Michigan',\n",
       " 'kearney': 'Nebraska',\n",
       " 'coralville': 'Iowa',\n",
       " 'bethel': 'Maine',\n",
       " 'boston ma': 'Massachusetts',\n",
       " \"**too small a sample size, can't share\": 'California',\n",
       " 'riverdale': 'Utah',\n",
       " 'orem': 'Utah',\n",
       " 'abington': 'Pennsylvania',\n",
       " 'grand haven': 'Michigan',\n",
       " 'watsonville': 'California',\n",
       " 'north chicago': 'Illinois',\n",
       " 'greenfield, ma': 'Vermont',\n",
       " 'home': 'Connecticut',\n",
       " 'n/a cloud-based': 'Pennsylvania',\n",
       " 'orange county- san diego county': 'Arizona, California',\n",
       " 'sanford': 'North Carolina',\n",
       " 'falmouth': 'Massachusetts',\n",
       " 'kenosha': 'Wisconsin',\n",
       " 'virginia beach': 'Virginia',\n",
       " 'puyallup': 'Washington',\n",
       " 'state college': 'Pennsylvania',\n",
       " 'mountainside': 'New Jersey',\n",
       " 'traverse city': 'Michigan',\n",
       " 'beaumont': 'Texas',\n",
       " 'norman,ok': 'Oklahoma',\n",
       " 'lewisville': 'Texas',\n",
       " 'waterville': 'Maine',\n",
       " 'newport news': 'Virginia',\n",
       " 'beverly': 'Massachusetts',\n",
       " 'cullowhee': 'North Carolina',\n",
       " 'iselin': 'New Jersey',\n",
       " 'portland, me area': 'Maine',\n",
       " 'monrovia': 'California',\n",
       " 'houston, but the job is remote': 'Texas',\n",
       " 'sauk centre': 'Minnesota',\n",
       " 'bathesda': 'Maryland',\n",
       " '': 'Massachusetts',\n",
       " 'henderson': 'Nevada',\n",
       " 'carbondale': 'Pennsylvania',\n",
       " 'remote position': 'Nevada',\n",
       " 'lodi': 'California',\n",
       " 'boerne': 'Texas',\n",
       " 'sf': 'California',\n",
       " 'easthampton': 'Massachusetts',\n",
       " 'holmdel': 'New Jersey',\n",
       " 'green bay': 'Wisconsin',\n",
       " 'roswell': 'Georgia',\n",
       " 'northern va': 'Virginia',\n",
       " 'remote (org based out of dc metro area)': 'Virginia',\n",
       " 'north of boston': 'Massachusetts',\n",
       " 'sandy': 'Utah',\n",
       " 'cupertino': 'California',\n",
       " 'major state metro area': 'North Carolina',\n",
       " 'glen allen': 'Virginia',\n",
       " 'derby, ct': 'Connecticut',\n",
       " 'shreveport': 'Louisiana',\n",
       " 'woonsocket': 'Rhode Island',\n",
       " 'martinez (bay area)': 'California',\n",
       " 'harrisburg area': 'Pennsylvania',\n",
       " 'fort lauderdale': 'Florida',\n",
       " 'highland heights': 'Kentucky',\n",
       " 'not disclosing': 'New Hampshire',\n",
       " 'brick': 'New Jersey',\n",
       " 'lawrenceburg': 'Kentucky',\n",
       " 'salina': 'Kansas',\n",
       " 'germantown': 'Maryland',\n",
       " 'nyc - queens': 'New York',\n",
       " 'fairfax county': 'Virginia',\n",
       " 'williamsport': 'Pennsylvania',\n",
       " 'hampton': 'Virginia',\n",
       " 'wooster': 'Ohio',\n",
       " 'birmingham, al': 'Alabama',\n",
       " 'wilmington, nc': 'North Carolina',\n",
       " 'charles town': 'West Virginia',\n",
       " 'raymond': 'New Hampshire',\n",
       " 'gaitherburg': 'Maryland',\n",
       " 'chevy chase': 'Maryland',\n",
       " 'rosslyn': 'Virginia',\n",
       " 'sunrise': 'Florida',\n",
       " 'renton': 'Washington',\n",
       " 'would rather not answer ( would be very easy to figure out who i am)': 'Maryland',\n",
       " 'alameda': 'California',\n",
       " 'remote; based out of hudson': 'Ohio, Wyoming',\n",
       " 'rural area': 'Ohio',\n",
       " 'zion': 'Illinois',\n",
       " 'bangor': 'Maine',\n",
       " 'moncks corner': 'South Carolina',\n",
       " 'milwaukee metro area': 'Wisconsin',\n",
       " 'fargo': 'North Dakota',\n",
       " 'lorton': 'Virginia',\n",
       " 'findlay, oh': 'Ohio',\n",
       " 'roseland': 'New Jersey',\n",
       " 'college station': 'Texas',\n",
       " 'a suburb of boston': 'Massachusetts',\n",
       " 'mcdonough': 'Georgia',\n",
       " 'pooler, ga': 'Georgia',\n",
       " 'seattle, washington': 'Washington',\n",
       " 'oregon': 'Wisconsin',\n",
       " 'twin cities metro': 'Minnesota',\n",
       " 'franklin springs': 'Georgia',\n",
       " 'bend, or': 'Oregon',\n",
       " 'd.c.': 'District of Columbia',\n",
       " 'harper woods': 'Michigan',\n",
       " 'glen burnie': 'Maryland',\n",
       " 'evansville': 'Indiana',\n",
       " 'pottstown': 'Pennsylvania',\n",
       " 'fairfield county': 'Connecticut',\n",
       " 'belmont, ca (sf bay area)': 'California',\n",
       " 'ogden': 'Utah',\n",
       " 'redlands': 'California',\n",
       " 'stone mountain': 'Georgia',\n",
       " 'ft lauderdale': 'Florida',\n",
       " 'mansfield pa': 'Pennsylvania',\n",
       " 'bloomington, in': 'Indiana',\n",
       " 'plymouth meeting': 'Pennsylvania',\n",
       " 'oregon city': 'Massachusetts, Oregon',\n",
       " 'brookston': 'Indiana',\n",
       " 'marshall': 'Texas',\n",
       " 'company is based in nashville, i work remotely from nyc': 'Tennessee',\n",
       " 'needham': 'Massachusetts',\n",
       " 'cranston, ri': 'Rhode Island',\n",
       " 'henrico': 'Virginia',\n",
       " 'geismar, la': 'Louisiana',\n",
       " 'arcadia': 'Wisconsin',\n",
       " 'carmel': 'Indiana',\n",
       " 'southern maryland': 'Maryland',\n",
       " 'virgina beach': 'Virginia',\n",
       " 'oshkosh': 'Wisconsin',\n",
       " 'pueblo': 'Colorado',\n",
       " 'work across the whole state': 'Oregon',\n",
       " 'see above': 'District of Columbia',\n",
       " 'folsom': 'California',\n",
       " 'santa barbara': 'California',\n",
       " 'schaumburg (chicago metro area)': 'Illinois',\n",
       " 'new york city/manhattan': 'New York',\n",
       " 'near ann arbor': 'Michigan',\n",
       " 'schenectady, ny': 'New York',\n",
       " 'northern co': 'Colorado',\n",
       " 'nyc (but i work remote)': 'New York',\n",
       " 'taunton': 'Massachusetts',\n",
       " 'greenwich': 'Connecticut',\n",
       " 'st clair shores': 'Michigan',\n",
       " 'wright': 'Wyoming',\n",
       " 'allen, tx': 'Texas',\n",
       " 'north wales': 'Pennsylvania',\n",
       " 'king of prussia': 'Pennsylvania',\n",
       " 'minneapolis, wfh': 'Minnesota',\n",
       " 'rotterdam': 'New York',\n",
       " 'bemidji': 'Minnesota',\n",
       " 'dont want to say': 'New Jersey',\n",
       " 'fallon': 'Nevada',\n",
       " 'cincinnati, oh': 'Ohio',\n",
       " 'benton': 'Arkansas',\n",
       " 'torrance, ca': 'California',\n",
       " 'lakeland': 'Florida',\n",
       " 'kansas city, mo': 'Kansas',\n",
       " 'towson': 'Maryland',\n",
       " 'upper valley': 'New Hampshire',\n",
       " 'fairbanks': 'Alaska',\n",
       " 'cartersville': 'Georgia',\n",
       " 'hillsdale': 'Michigan',\n",
       " 'portland, me': 'Maine',\n",
       " 'a city small enough to not answer this question': 'Pennsylvania',\n",
       " 'carrollton': 'Texas',\n",
       " 'petaluma': 'California',\n",
       " 'brambleton': 'Virginia',\n",
       " 'wilkes-barre': 'Pennsylvania',\n",
       " 'marquette': 'Michigan',\n",
       " 'iowa city': 'Iowa',\n",
       " 'greater cleveland': 'Ohio',\n",
       " 'federal way': 'Washington',\n",
       " 'mckinney': 'Texas',\n",
       " 'kansas city region': 'Missouri',\n",
       " 'rural new england': 'New Hampshire',\n",
       " 'columbia md': 'Maryland',\n",
       " 'escondido': 'California',\n",
       " 'wfh kenilworth': 'New Jersey',\n",
       " 'kirkland': 'Washington',\n",
       " 'stillwater: wi based company, office in mn': 'Minnesota',\n",
       " 'issaquah': 'Washington',\n",
       " 'takoma park, md': 'Maryland',\n",
       " 'dc area': 'Maryland',\n",
       " 'santa monica': 'California',\n",
       " 'medium size city': 'Missouri',\n",
       " 'trinidad': 'Colorado',\n",
       " 'long beach': 'California',\n",
       " 'fraser': 'Colorado',\n",
       " 'san mateo': 'California',\n",
       " 'central pa': 'Pennsylvania',\n",
       " 'corpus christi': 'Texas',\n",
       " 'suburb of philadelphia': 'Pennsylvania',\n",
       " 'fort eustis': 'Virginia',\n",
       " 'osceola': 'Arkansas',\n",
       " 'edmonds': 'Washington',\n",
       " 'tucson, az': 'Arizona',\n",
       " 'plattsburgh': 'New York',\n",
       " 'camp verde': 'Arizona',\n",
       " 'near indianapolis': 'Indiana',\n",
       " 'noblesville': 'Indiana',\n",
       " 'tupelo': 'Mississippi',\n",
       " 'waukesha': 'Wisconsin',\n",
       " 'champaign urbana': 'Illinois',\n",
       " 'oakland ca': 'California',\n",
       " 'medford oregon': 'Oregon',\n",
       " 'lake jackson': 'Texas',\n",
       " 'baltimore (but i work fully remotely)': 'Maryland',\n",
       " 'minneapolis-st paul': 'Minnesota',\n",
       " 'paso robles': 'California',\n",
       " 'los angles': 'California',\n",
       " 'hamlet': 'North Carolina',\n",
       " 'hammond': 'Louisiana',\n",
       " 'chicagoland area': 'Illinois',\n",
       " 'palo alto': 'California',\n",
       " 'raeford': 'North Carolina',\n",
       " 'the woodlands': 'Texas',\n",
       " 'groton': 'Connecticut',\n",
       " 'couoeville': 'Washington',\n",
       " 'kingsburg': 'California',\n",
       " 'riverview': 'Florida',\n",
       " 'scranton/wilkes-barre': 'Pennsylvania',\n",
       " 'west bend': 'Wisconsin',\n",
       " 'greater seattle area': 'Washington',\n",
       " 'new york city/brooklyn': 'New York',\n",
       " 'aiken': 'South Carolina',\n",
       " 'dallas-ft worth': 'Texas',\n",
       " 'a small city': 'Iowa',\n",
       " 'clifton': 'New Jersey',\n",
       " 'waltham, ma': 'Massachusetts',\n",
       " 'santa clarita': 'California',\n",
       " 'honolilu': 'Hawaii',\n",
       " 'leavenworth': 'Kansas',\n",
       " 'madison, wi': 'Wisconsin',\n",
       " 'keene': 'New Hampshire',\n",
       " 'joplin': 'Missouri',\n",
       " 'allendale, mi': 'Michigan',\n",
       " 'cherry hill, nj': 'Pennsylvania',\n",
       " 'paragould': 'Arkansas',\n",
       " 'san bruno': 'California',\n",
       " 'sheffield': 'Massachusetts',\n",
       " 'grinnell': 'Iowa',\n",
       " 'west roxbury': 'Massachusetts',\n",
       " 'northern vermont': 'Vermont',\n",
       " 'dekalb': 'Illinois',\n",
       " 'rock hill': 'South Carolina',\n",
       " 'helena': 'Montana',\n",
       " 'nanticoke': 'Pennsylvania',\n",
       " 'plantation': 'Florida',\n",
       " 'beachwood': 'Ohio',\n",
       " 'greeley': 'Colorado',\n",
       " 'socal, prefer not to specify': 'California',\n",
       " 'austin metro area': 'Texas',\n",
       " 'florham park': 'New Jersey',\n",
       " 'elizabeth': 'New Jersey',\n",
       " 'brookfield': 'Wisconsin',\n",
       " 'central il': 'Illinois',\n",
       " 'mahwah': 'New Jersey',\n",
       " 'fort knox': 'Kentucky',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_state_mapping = {\n",
    "    city: max(states.items(), key=lambda x: x[1])[0]\n",
    "    for city, states in city_state_map.items()\n",
    "}\n",
    "\n",
    "city_state_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a846991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing states for US records before: 171\n"
     ]
    }
   ],
   "source": [
    "mask = (\n",
    "    (df['Country_Work'] == 'United States') & \n",
    "    (df['State_Work_USA'].isna()) & \n",
    "    (df['City_Work'].notna())\n",
    ")\n",
    "\n",
    "\n",
    "# Count before\n",
    "print(\"Missing states for US records before:\", df[\n",
    "    (df['Country_Work'] == 'United States') & \n",
    "    (df['State_Work_USA'].isna())\n",
    "].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Apply the mapping\n",
    "for idx in df[mask].index:\n",
    "    city = df.loc[idx, 'City_Work'].strip().lower()\n",
    "    if city in city_state_mapping:\n",
    "        df.loc[idx, 'State_Work_USA'] = city_state_mapping[city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6240c8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing states for US records after: 44\n",
      "\n",
      "Example of filled state values:\n",
      "         City_Work State_Work_USA\n",
      "21863     Freeport       Illinois\n",
      "9439      New York       New York\n",
      "4592   Minneapolis      Minnesota\n",
      "24557      Houston          Texas\n",
      "2133      Columbus           Ohio\n"
     ]
    }
   ],
   "source": [
    "# Count after\n",
    "print(\"Missing states for US records after:\", df[\n",
    "    (df['Country_Work'] == 'United States') & \n",
    "    (df['State_Work_USA'].isna())\n",
    "].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Show some examples of filled values\n",
    "print(\"\\nExample of filled state values:\")\n",
    "filled_examples = df[\n",
    "    (df['Country_Work'] == 'United States') & \n",
    "    (df['State_Work_USA'].notna()) & \n",
    "    (df['City_Work'].notna())\n",
    "].sample(5)\n",
    "\n",
    "\n",
    "print(filled_examples[['City_Work', 'State_Work_USA']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e539139-1ced-4772-95a0-60d58bb72f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract numeric ranges from experience strings\n",
    "def extract_years_range(experience_str):\n",
    "    if pd.isna(experience_str):\n",
    "        return pd.NA, pd.NA\n",
    "    \n",
    "    experience_str = str(experience_str).lower().strip()\n",
    "    \n",
    "    # Handle special cases\n",
    "    if experience_str in ['< 1 year', '1 year or less']:\n",
    "        return 0, 1\n",
    "    \n",
    "    elif '41 years or more' in experience_str:\n",
    "        return 41, 50 \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    numbers = [int(s) for s in experience_str.replace('years', '').split('-') if s.strip().isdigit()]\n",
    "    \n",
    "    if len(numbers) == 2:\n",
    "        return numbers[0], numbers[1]\n",
    "    elif len(numbers) == 1:\n",
    "        if '31 - 40' in experience_str:\n",
    "            return 31, 40\n",
    "        elif '21 - 30' in experience_str:\n",
    "            return 21, 30\n",
    "        elif '41' in experience_str:\n",
    "            return 41, 50\n",
    "        else:\n",
    "            return numbers[0], numbers[0]\n",
    "    return pd.NA, pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e67f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for experience ranges\n",
    "df[['Total_Years_Min', 'Total_Years_Max']] = df.apply(\n",
    "    lambda x: pd.Series(extract_years_range(x['Total_Years_of_Experience'])), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[['Related_Years_Min', 'Related_Years_Max']] = df.apply(\n",
    "    lambda x: pd.Series(extract_years_range(x['Related_Work_Experience'])), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fb27375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract age ranges with additional handling\n",
    "def extract_age_range(age_str):\n",
    "    if pd.isna(age_str):\n",
    "        return pd.NA, pd.NA\n",
    "    \n",
    "    age_str = str(age_str).lower().strip()\n",
    "    \n",
    "    # Handle special cases\n",
    "    if 'under 18' in age_str:\n",
    "        return 0, 17\n",
    "    elif '65' in age_str or 'over' in age_str:\n",
    "        return 65, 100  # Using 100 as an upper bound\n",
    "    \n",
    "    # Extract numbers from strings like \"25-34\", \"35-44\" etc.\n",
    "    numbers = [int(s) for s in age_str.split('-') if s.isdigit()]\n",
    "    \n",
    "    if len(numbers) == 2:\n",
    "        return numbers[0], numbers[1]\n",
    "    return pd.NA, pd.NA\n",
    "\n",
    "# Create new columns for age ranges\n",
    "df[['Age_Min', 'Age_Max']] = df.apply(\n",
    "    lambda x: pd.Series(extract_age_range(x['age'])), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c456db6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Age Categories Distribution:\n",
      "age\n",
      "25-34         12668\n",
      "35-44          9908\n",
      "45-54          3193\n",
      "18-24          1236\n",
      "55-64           994\n",
      "65 or over       95\n",
      "under 18         14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Age Range Statistics:\n",
      "            Age_Min       Age_Max\n",
      "count  28108.000000  28108.000000\n",
      "mean      31.672762     40.632702\n",
      "std        8.710147      9.369640\n",
      "min        0.000000     17.000000\n",
      "25%       25.000000     34.000000\n",
      "50%       35.000000     44.000000\n",
      "75%       35.000000     44.000000\n",
      "max       65.000000    100.000000\n",
      "\n",
      "Experience Categories Distribution:\n",
      "Total_Years_of_Experience\n",
      "11 - 20 years       9630\n",
      "8 - 10 years        5381\n",
      "5-7 years           4886\n",
      "21 - 30 years       3645\n",
      "2 - 4 years         3038\n",
      "31 - 40 years        870\n",
      "1 year or less       533\n",
      "41 years or more     125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total Years Experience Range Statistics:\n",
      "       Total_Years_Min  Total_Years_Max\n",
      "count     28108.000000     28108.000000\n",
      "mean         10.250605        15.785435\n",
      "std           6.915288         9.727685\n",
      "min           0.000000         1.000000\n",
      "25%           5.000000         7.000000\n",
      "50%          11.000000        20.000000\n",
      "75%          11.000000        20.000000\n",
      "max          41.000000        50.000000\n",
      "\n",
      "Null Values in Transformed Columns:\n",
      "Age_Min            0\n",
      "Age_Max            0\n",
      "Total_Years_Min    0\n",
      "Total_Years_Max    0\n",
      "dtype: int64\n",
      "\n",
      "Unique Experience Categories:\n",
      "['1 year or less', '11 - 20 years', '2 - 4 years', '21 - 30 years', '31 - 40 years', '41 years or more', '5-7 years', '8 - 10 years']\n"
     ]
    }
   ],
   "source": [
    "# Print detailed statistics to verify transformations\n",
    "print(\"\\nAge Categories Distribution:\")\n",
    "print(df['age'].value_counts())\n",
    "print(\"\\nAge Range Statistics:\")\n",
    "print(df[['Age_Min', 'Age_Max']].describe())\n",
    "\n",
    "print(\"\\nExperience Categories Distribution:\")\n",
    "print(df['Total_Years_of_Experience'].value_counts().head(10))\n",
    "print(\"\\nTotal Years Experience Range Statistics:\")\n",
    "print(df[['Total_Years_Min', 'Total_Years_Max']].describe())\n",
    "\n",
    "# Check for any remaining null values\n",
    "print(\"\\nNull Values in Transformed Columns:\")\n",
    "print(df[['Age_Min', 'Age_Max', 'Total_Years_Min', 'Total_Years_Max']].isnull().sum())\n",
    "\n",
    "# Print unique values to verify all categories are captured\n",
    "print(\"\\nUnique Experience Categories:\")\n",
    "print(sorted(df['Total_Years_of_Experience'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2639e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                        0\n",
       "age                              0\n",
       "Work_Industry                   75\n",
       "Job_Title                        1\n",
       "Job_Title_Add_Comments       20835\n",
       "Annual_Salary                    0\n",
       "Bonus                         7315\n",
       "Currency                         0\n",
       "Currency_Other               27897\n",
       "Income_Context               25061\n",
       "Country_Work                    78\n",
       "State_Work_USA                4907\n",
       "City_Work                       82\n",
       "Total_Years_of_Experience        0\n",
       "Related_Work_Experience          0\n",
       "Highest_Qualification          223\n",
       "Gender                         171\n",
       "Race                           177\n",
       "Total_Years_Min                  0\n",
       "Total_Years_Max                  0\n",
       "Related_Years_Min                0\n",
       "Related_Years_Max                0\n",
       "Age_Min                          0\n",
       "Age_Max                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a5c6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>age</th>\n",
       "      <th>Work_Industry</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Title_Add_Comments</th>\n",
       "      <th>Annual_Salary</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Currency_Other</th>\n",
       "      <th>Income_Context</th>\n",
       "      <th>...</th>\n",
       "      <th>Related_Work_Experience</th>\n",
       "      <th>Highest_Qualification</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Total_Years_Min</th>\n",
       "      <th>Total_Years_Max</th>\n",
       "      <th>Related_Years_Min</th>\n",
       "      <th>Related_Years_Max</th>\n",
       "      <th>Age_Min</th>\n",
       "      <th>Age_Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24562</th>\n",
       "      <td>5/5/2021 16:24:23</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Administrative Assistant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11 - 20 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6792</th>\n",
       "      <td>4/27/2021 14:09:11</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Media &amp; Digital</td>\n",
       "      <td>Assistant Production Editor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>Asian or Asian American, White</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>4/27/2021 14:01:31</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Computing or Tech</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11242</th>\n",
       "      <td>4/27/2021 23:55:45</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Energy, Oil &amp; Gas</td>\n",
       "      <td>Director of Contracts &amp; Risk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130000</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21 - 30 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10731</th>\n",
       "      <td>4/27/2021 22:03:29</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Hospitality &amp; Events</td>\n",
       "      <td>Event Monitor</td>\n",
       "      <td>Work in a convention center setting up, taking...</td>\n",
       "      <td>28000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>High School</td>\n",
       "      <td>Man</td>\n",
       "      <td>White</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    age                 Work_Industry  \\\n",
       "24562   5/5/2021 16:24:23  35-44  Education (Higher Education)   \n",
       "6792   4/27/2021 14:09:11  25-34               Media & Digital   \n",
       "6671   4/27/2021 14:01:31  25-34             Computing or Tech   \n",
       "11242  4/27/2021 23:55:45  45-54             Energy, Oil & Gas   \n",
       "10731  4/27/2021 22:03:29  35-44          Hospitality & Events   \n",
       "\n",
       "                          Job_Title  \\\n",
       "24562      Administrative Assistant   \n",
       "6792    Assistant Production Editor   \n",
       "6671              Software Engineer   \n",
       "11242  Director of Contracts & Risk   \n",
       "10731                 Event Monitor   \n",
       "\n",
       "                                  Job_Title_Add_Comments  Annual_Salary  \\\n",
       "24562                                                NaN          42000   \n",
       "6792                                                 NaN          41000   \n",
       "6671                                                 NaN          90000   \n",
       "11242                                                NaN         130000   \n",
       "10731  Work in a convention center setting up, taking...          28000   \n",
       "\n",
       "         Bonus Currency Currency_Other Income_Context  ...  \\\n",
       "24562      NaN      USD            NaN            NaN  ...   \n",
       "6792    1000.0      USD            NaN            NaN  ...   \n",
       "6671       NaN      CAD            NaN            NaN  ...   \n",
       "11242  15000.0      USD            NaN            NaN  ...   \n",
       "10731   2000.0      USD            NaN            NaN  ...   \n",
       "\n",
       "      Related_Work_Experience Highest_Qualification Gender  \\\n",
       "24562           11 - 20 years        College degree  Woman   \n",
       "6792              2 - 4 years        College degree  Woman   \n",
       "6671                5-7 years        College degree  Woman   \n",
       "11242           21 - 30 years       Master's degree  Woman   \n",
       "10731             2 - 4 years           High School    Man   \n",
       "\n",
       "                                 Race Total_Years_Min Total_Years_Max  \\\n",
       "24562                           White              11              20   \n",
       "6792   Asian or Asian American, White               2               4   \n",
       "6671                            White               8              10   \n",
       "11242                           White              21              30   \n",
       "10731                           White              21              30   \n",
       "\n",
       "      Related_Years_Min Related_Years_Max  Age_Min  Age_Max  \n",
       "24562                11                20       35       44  \n",
       "6792                  2                 4       25       34  \n",
       "6671                  5                 7       25       34  \n",
       "11242                21                30       45       54  \n",
       "10731                 2                 4       35       44  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c44a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping columns:\n",
      "Index(['timestamp', 'age', 'Work_Industry', 'Job_Title',\n",
      "       'Job_Title_Add_Comments', 'Annual_Salary', 'Bonus', 'Currency',\n",
      "       'Currency_Other', 'Income_Context', 'Country_Work', 'State_Work_USA',\n",
      "       'City_Work', 'Total_Years_of_Experience', 'Related_Work_Experience',\n",
      "       'Highest_Qualification', 'Gender', 'Race', 'Total_Years_Min',\n",
      "       'Total_Years_Max', 'Related_Years_Min', 'Related_Years_Max', 'Age_Min',\n",
      "       'Age_Max'],\n",
      "      dtype='object')\n",
      "\n",
      "After dropping columns:\n",
      "Index(['timestamp', 'Work_Industry', 'Job_Title', 'Job_Title_Add_Comments',\n",
      "       'Annual_Salary', 'Bonus', 'Currency', 'Currency_Other',\n",
      "       'Income_Context', 'Country_Work', 'State_Work_USA', 'City_Work',\n",
      "       'Highest_Qualification', 'Gender', 'Race', 'Total_Years_Min',\n",
      "       'Total_Years_Max', 'Related_Years_Min', 'Related_Years_Max', 'Age_Min',\n",
      "       'Age_Max'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# First verify the new columns have been created correctly\n",
    "print(\"Before dropping columns:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Drop original columns while keeping the new numeric range columns\n",
    "columns_to_drop = ['age', 'Total_Years_of_Experience', 'Related_Work_Experience']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"\\nAfter dropping columns:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37823658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified gender standardization based on the actual survey options\n",
    "def standardize_gender(gender):\n",
    "    if pd.isna(gender):\n",
    "        return 'Other or prefer not to answer'\n",
    "    \n",
    "    gender = str(gender).strip()\n",
    "    \n",
    "    # Map to only the survey options\n",
    "    if gender in ['Man', 'Woman', 'Non-binary']:\n",
    "        return gender\n",
    "    else:\n",
    "        return 'Other or prefer not to answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4494d5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before standardization:\n",
      "Gender\n",
      "Woman                            21389\n",
      "Man                               5502\n",
      "Non-binary                         747\n",
      "Other or prefer not to answer      298\n",
      "NaN                                171\n",
      "Prefer not to answer                 1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After standardization:\n",
      "Gender_Standardized\n",
      "Woman                            21389\n",
      "Man                               5502\n",
      "Non-binary                         747\n",
      "Other or prefer not to answer      470\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final Gender Distribution:\n",
      "Gender\n",
      "Woman                            21389\n",
      "Man                               5502\n",
      "Non-binary                         747\n",
      "Other or prefer not to answer      470\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Null Value Count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new column with standardized categories\n",
    "df['Gender_Standardized'] = df['Gender'].apply(standardize_gender)\n",
    "\n",
    "# Print before and after\n",
    "print(\"Before standardization:\")\n",
    "print(df['Gender'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(df['Gender_Standardized'].value_counts(dropna=False))\n",
    "\n",
    "# After verification, update the gender column\n",
    "df = df.drop(columns=['Gender'])\n",
    "df = df.rename(columns={'Gender_Standardized': 'Gender'})\n",
    "\n",
    "print(\"\\nFinal Gender Distribution:\")\n",
    "print(df['Gender'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nNull Value Count:\")\n",
    "df.Gender.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e89fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial missing bonus values: 7315\n",
      "\n",
      "Final missing bonus values: 0\n",
      "\n",
      "Bonus distribution after filling zeros:\n",
      "count    2.810800e+04\n",
      "mean     1.349651e+04\n",
      "std      7.170322e+05\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      5.000000e+03\n",
      "max      1.200000e+08\n",
      "Name: Bonus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print initial missing bonus count\n",
    "print(\"Initial missing bonus values:\", df['Bonus'].isnull().sum())\n",
    "\n",
    "# Replace all missing bonus values with 0\n",
    "df['Bonus'] = df['Bonus'].fillna(0)\n",
    "\n",
    "# Print final missing bonus count and distribution\n",
    "print(\"\\nFinal missing bonus values:\", df['Bonus'].isnull().sum())\n",
    "print(\"\\nBonus distribution after filling zeros:\")\n",
    "print(df['Bonus'].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df0c5312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'Work_Industry', 'Job_Title', 'Job_Title_Add_Comments',\n",
       "       'Annual_Salary', 'Bonus', 'Currency', 'Currency_Other',\n",
       "       'Income_Context', 'Country_Work', 'State_Work_USA', 'City_Work',\n",
       "       'Highest_Qualification', 'Race', 'Total_Years_Min', 'Total_Years_Max',\n",
       "       'Related_Years_Min', 'Related_Years_Max', 'Age_Min', 'Age_Max',\n",
       "       'Gender'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77364b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversion rates\n",
    "conversion_rates = {\n",
    "    'USD': 1.0, \n",
    "    'GBP': 1.35, \n",
    "    'CAD': 0.79, \n",
    "    'EUR': 1.12, \n",
    "    'AUD': 0.74, \n",
    "    'NZD': 0.71, \n",
    "    'CHF': 1.08, \n",
    "    'ZAR': 0.065, \n",
    "    'SEK': 0.11, \n",
    "    'HKD': 0.13, \n",
    "    'JPY': 0.0091\n",
    "}\n",
    "\n",
    "# Function to normalize monetary values to USD\n",
    "def normalize_to_usd(row, column):\n",
    "    try:\n",
    "        # Clean the value by removing commas\n",
    "        value = float(str(row[column]).replace(',', ''))\n",
    "        currency = row['Currency']\n",
    "        \n",
    "\n",
    "\n",
    "        # If currency exists in our conversion rates, use it\n",
    "        if currency in conversion_rates:\n",
    "            return value * conversion_rates[currency]\n",
    "        \n",
    "        else:\n",
    "            # If currency not in our list, assume USD\n",
    "            return value\n",
    "            \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "469a3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns with normalized values\n",
    "df['Salary_USD'] = df.apply(lambda row: normalize_to_usd(row, 'Annual_Salary'), axis=1)\n",
    "\n",
    "df['Bonus_USD'] = df.apply(lambda row: normalize_to_usd(row, 'Bonus'), axis=1)\n",
    "\n",
    "# Fill NA values in Bonus_USD with 0\n",
    "df['Bonus_USD'] = df['Bonus_USD'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63406be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create salary and bonus outlier flags based on industry-specific IQR\n",
    "def flag_outliers(df, column):\n",
    "    flag_column = f'{column}_Outlier_Flag'\n",
    "    df[flag_column] = False\n",
    "    \n",
    "\n",
    "    for industry in df['Work_Industry'].unique():\n",
    "        if pd.isna(industry):\n",
    "            continue\n",
    "            \n",
    "        industry_values = df[df['Work_Industry'] == industry][column]\n",
    "        Q1 = industry_values.quantile(0.25)\n",
    "        Q3 = industry_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        industry_mask = df['Work_Industry'] == industry\n",
    "        outlier_mask = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "        df.loc[industry_mask & outlier_mask, flag_column] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "716d9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply outlier flagging for both salary and bonus\n",
    "flag_outliers(df, 'Salary_USD')\n",
    "flag_outliers(df, 'Bonus_USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f5968fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique race values from the dataset\n",
    "unique_races = set()\n",
    "for race_str in df['Race'].dropna():\n",
    "    # Split by comma and strip whitespace\n",
    "    races = [r.strip() for r in str(race_str).split(',')]\n",
    "    unique_races.update(races)\n",
    "\n",
    "# Create binary columns for each unique race value\n",
    "for race in sorted(unique_races):\n",
    "    column_name = f'Race_{race.replace(\" \", \"_\")}'\n",
    "    df[column_name] = df['Race'].apply(\n",
    "        lambda x: 1 if pd.notna(x) and race in str(x) else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5edb5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race Distribution:\n",
      "Another option not listed here or prefer not to answer: 725 (2.6%)\n",
      "Asian or Asian American: 1,833 (6.5%)\n",
      "Black or African American: 895 (3.2%)\n",
      "Hispanic: 1,101 (3.9%)\n",
      "Latino: 1,101 (3.9%)\n",
      "Middle Eastern or Northern African: 182 (0.6%)\n",
      "Native American or Alaska Native: 157 (0.6%)\n",
      "White: 24,384 (86.8%)\n",
      "or Spanish origin: 1,101 (3.9%)\n",
      "\n",
      "Multiple Races: 1,853 (6.6%)\n"
     ]
    }
   ],
   "source": [
    "# Create Multiple_Races flag\n",
    "race_columns = [col for col in df.columns if col.startswith('Race_')]\n",
    "df['Multiple_Races'] = (df[race_columns].sum(axis=1) > 1).astype(int)\n",
    "\n",
    "# Print summary\n",
    "print(\"Race Distribution:\")\n",
    "total_respondents = len(df)\n",
    "for col in sorted(race_columns):\n",
    "    count = df[col].sum()\n",
    "    percentage = (count / total_respondents) * 100\n",
    "    print(f\"{col.replace('Race_', '').replace('_', ' ')}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMultiple Races: {df['Multiple_Races'].sum():,} ({(df['Multiple_Races'].sum() / total_respondents) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b5ba191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Work_Industry</th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Title_Add_Comments</th>\n",
       "      <th>Annual_Salary</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Currency_Other</th>\n",
       "      <th>Income_Context</th>\n",
       "      <th>Country_Work</th>\n",
       "      <th>...</th>\n",
       "      <th>Race_Another_option_not_listed_here_or_prefer_not_to_answer</th>\n",
       "      <th>Race_Asian_or_Asian_American</th>\n",
       "      <th>Race_Black_or_African_American</th>\n",
       "      <th>Race_Hispanic</th>\n",
       "      <th>Race_Latino</th>\n",
       "      <th>Race_Middle_Eastern_or_Northern_African</th>\n",
       "      <th>Race_Native_American_or_Alaska_Native</th>\n",
       "      <th>Race_White</th>\n",
       "      <th>Race_or_Spanish_origin</th>\n",
       "      <th>Multiple_Races</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11065</th>\n",
       "      <td>4/27/2021 23:10:39</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Product</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110000</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22289</th>\n",
       "      <td>4/30/2021 18:55:20</td>\n",
       "      <td>Engineering or Manufacturing</td>\n",
       "      <td>Technical services scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18584</th>\n",
       "      <td>4/29/2021 1:01:02</td>\n",
       "      <td>Business or Consulting</td>\n",
       "      <td>Project Manager</td>\n",
       "      <td>Real estate, nonprofits, and venture capital</td>\n",
       "      <td>90000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23930</th>\n",
       "      <td>5/3/2021 18:43:49</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Instructional Designer</td>\n",
       "      <td>My actual work in as a Project Manager</td>\n",
       "      <td>64000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24879</th>\n",
       "      <td>5/5/2021 21:18:41</td>\n",
       "      <td>Government and Public Administration</td>\n",
       "      <td>Research scientist</td>\n",
       "      <td>I work for a university but my work site is a ...</td>\n",
       "      <td>76000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp                         Work_Industry  \\\n",
       "11065  4/27/2021 23:10:39         Accounting, Banking & Finance   \n",
       "22289  4/30/2021 18:55:20          Engineering or Manufacturing   \n",
       "18584   4/29/2021 1:01:02                Business or Consulting   \n",
       "23930   5/3/2021 18:43:49          Education (Higher Education)   \n",
       "24879   5/5/2021 21:18:41  Government and Public Administration   \n",
       "\n",
       "                           Job_Title  \\\n",
       "11065                        Product   \n",
       "22289  Technical services scientist    \n",
       "18584                Project Manager   \n",
       "23930        Instructional Designer    \n",
       "24879            Research scientist    \n",
       "\n",
       "                                  Job_Title_Add_Comments  Annual_Salary  \\\n",
       "11065                                                NaN         110000   \n",
       "22289                                                NaN          95000   \n",
       "18584       Real estate, nonprofits, and venture capital          90000   \n",
       "23930             My actual work in as a Project Manager          64000   \n",
       "24879  I work for a university but my work site is a ...          76000   \n",
       "\n",
       "         Bonus Currency Currency_Other Income_Context   Country_Work  ...  \\\n",
       "11065  25000.0      USD            NaN            NaN  United States  ...   \n",
       "22289      0.0      USD            NaN            NaN  United States  ...   \n",
       "18584   2000.0      USD            NaN            NaN  United States  ...   \n",
       "23930      0.0      USD            NaN            NaN  United States  ...   \n",
       "24879      0.0      USD            NaN            NaN  United States  ...   \n",
       "\n",
       "      Race_Another_option_not_listed_here_or_prefer_not_to_answer  \\\n",
       "11065                                                  0            \n",
       "22289                                                  0            \n",
       "18584                                                  0            \n",
       "23930                                                  0            \n",
       "24879                                                  0            \n",
       "\n",
       "      Race_Asian_or_Asian_American Race_Black_or_African_American  \\\n",
       "11065                            0                              0   \n",
       "22289                            0                              0   \n",
       "18584                            0                              0   \n",
       "23930                            0                              0   \n",
       "24879                            0                              0   \n",
       "\n",
       "      Race_Hispanic  Race_Latino  Race_Middle_Eastern_or_Northern_African  \\\n",
       "11065             0            0                                        0   \n",
       "22289             0            0                                        0   \n",
       "18584             1            1                                        0   \n",
       "23930             0            0                                        0   \n",
       "24879             0            0                                        0   \n",
       "\n",
       "       Race_Native_American_or_Alaska_Native  Race_White  \\\n",
       "11065                                      0           1   \n",
       "22289                                      0           1   \n",
       "18584                                      0           0   \n",
       "23930                                      0           1   \n",
       "24879                                      0           1   \n",
       "\n",
       "       Race_or_Spanish_origin  Multiple_Races  \n",
       "11065                       0               0  \n",
       "22289                       0               0  \n",
       "18584                       1               1  \n",
       "23930                       0               0  \n",
       "24879                       0               0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49774589-0d4f-4423-bfe5-7a4e04310b2c",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "The salary survey dataset presented several major data quality challenges that required careful handling. The most significant issues included inconsistent data entry formats, missing values across multiple columns, and the presence of extreme outliers in salary and bonus fields. Free-form text entries led to high variability in fields like Job_Title (14,377 unique values) and Work_Industry (1,220 unique values), making standardization difficult. Geographic data was particularly messy, with multiple variations of country names (e.g., \"US\", \"USA\", \"United States\") and inconsistent city/state pairings.\n",
    "\n",
    "The cleaning process substantially improved the dataset's usability for machine learning in several ways. Converting categorical variables into structured numeric formats made them suitable for model input - for example, transforming experience ranges like \"5-7 years\" into minimum and maximum year values. Standardizing currencies to USD and handling outliers with industry-specific flags rather than removal preserved data while marking potential issues. The creation of binary columns for race categories enabled proper handling of multi-racial identifications without losing information. These transformations maintained the dataset's richness while making it processable by ML algorithms.\n",
    "\n",
    "Training a model on the uncleaned dataset would likely produce unreliable results. The messy data would introduce several problems: currency differences would distort salary comparisons, free-form text fields would create sparse feature spaces, and unhandled outliers could skew predictions. Geographic analysis would be particularly unreliable due to inconsistent location data. In contrast, the cleaned dataset enables more accurate modeling by providing normalized values, structured categories, and clear indicators for potential data quality issues.\n",
    "\n",
    "While cleaning the data, I made conscious choices to minimize bias introduction while making the data more usable. Instead of dropping records with missing values, I used contextual filling where appropriate (e.g., zero for missing bonuses) and created flags for uncertain data. For categorical variables like gender, I preserved all response types while standardizing their format. The handling of outliers through industry-specific flags rather than removal ensures that unusual but potentially valid data points remain in the dataset. However, some bias may have been introduced through the currency conversion process and the consolidation of job titles and industries. I attempted to mitigate this by documenting all transformations and preserving original values alongside standardized ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1b8cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
